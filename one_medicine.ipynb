{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the notebook, a model is trained for each medicine\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "the last few blocks were commmented since I have no clue what they were for.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the entire column related to a specific medicine if its occurrence is below the defined threshold.\n",
    "# In the given dataset, setting this threshold to 250 would retain columns for only the top 10 most frequently used medicines.\n",
    "DeleteMedThreshold = 250\n",
    "\n",
    "# Determine the number of medicines to be trained as output\n",
    "# The total number of medicine is 102\n",
    "NumMedTrain = 102\n",
    "\n",
    "# Decide whether to enhance accuracy by utilizing class weights\n",
    "UseClassWeight = True\n",
    "\n",
    "# Decide learning_rate\n",
    "LearningRate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing TensorFlow for deep learning\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "# Importing scikit-learn for data preprocessing and utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Importing custom utility functions\n",
    "from utility_file import my_utilities as myutil\n",
    "from utility_file import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read Data\n",
    "Load the data using the custom module \"load_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "ReadData:\n",
      "Type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n",
      "End of ReadData\n",
      "--------------------------------------------------------------------------------\n",
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols).\n",
      "Shape of y = (796 rows, 102 cols).\n",
      "End of SplitXY\n",
      "--------------------------------------------------------------------------------\n",
      "In load_data_for_1_med_with_debug of load_data.py, random_seed= 1\n",
      "After SplitXY, total number of 0, 1 in y:\n",
      "Number of 0s: 72318\n",
      "Number of 1s: 8874\n",
      "--------------------------------------------------------------------------------\n",
      "DeleteMedicine: shape of y is (796, 10).\n",
      "Train_X.shape:  (637, 111)\n",
      "Train_y.shape:  (637, 10)\n",
      "\n",
      "Split Training Validation\n",
      "Number of 0s in train_y: 3660\n",
      "Number of 1s train_y: 2710\n",
      "Number of 0s in val_y: 860\n",
      "Number of 1s val_y: 730\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load data for training a model with one specific medicine, including debugging information\n",
    "(X_np, X_val_np, train_y, val_y, \n",
    " num_col_x, num_1_valy, num_0_valy) = load_data.load_data_for_1_med_with_debug(del_med_thres=DeleteMedThreshold, random_seed=1, n=NumMedTrain)\n",
    "\n",
    "# Ensure the correct data types for loaded variables\n",
    "assert isinstance(X_np, np.ndarray)\n",
    "assert isinstance(X_val_np, np.ndarray)\n",
    "assert isinstance(train_y, pd.DataFrame)\n",
    "assert isinstance(val_y, pd.DataFrame)\n",
    "assert isinstance(num_col_x, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Type Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in y: 0\n",
      "Number of str values in y: 0\n",
      "Number of int values in y: 1590\n",
      "Number of float values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below to display the DataFrame content and structure\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "\n",
    "# Uncomment the line below to print the DataFrame directly\n",
    "# print(val_y)\n",
    "\n",
    "# Checking:\n",
    "\n",
    "# Counting NA values in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "\n",
    "# Counting str values in y\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "\n",
    "# Counting int values in y\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "\n",
    "# Counting float values in y\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0.42543171114599687, 1: 0.5745682888540031}, 1: {0: 0.3218210361067504, 1: 0.6781789638932496}, 2: {0: 0.39717425431711145, 1: 0.6028257456828885}, 3: {0: 0.32653061224489793, 1: 0.673469387755102}, 4: {0: 0.35478806907378335, 1: 0.6452119309262166}, 5: {0: 0.4207221350078493, 1: 0.5792778649921507}, 6: {0: 0.5667189952904239, 1: 0.43328100470957615}, 7: {0: 0.33124018838304553, 1: 0.6687598116169545}, 8: {0: 0.5117739403453689, 1: 0.48822605965463106}, 9: {0: 0.598116169544741, 1: 0.40188383045525905}}\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'train_y' DataFrame to a NumPy array\n",
    "train_y_np = np.array(train_y)\n",
    "\n",
    "# Determine the number of labels (columns) in the array\n",
    "num_labels = train_y_np.shape[1]\n",
    "\n",
    "# Initialize an empty dictionary to store class weights for each label\n",
    "class_weight_dic = {}\n",
    "\n",
    "# Iterate over each label column\n",
    "for i in range(num_labels):\n",
    "    # Count the occurrences of each class (0 and 1) in the current label column\n",
    "    unique_values, counts = np.unique(train_y_np[:, i], return_counts=True)\n",
    "    \n",
    "    # Create a dictionary mapping class values to their frequencies\n",
    "    value_frequency_dict = dict(zip(unique_values, counts))\n",
    "    \n",
    "    # Calculate the total number of occurrences for normalization\n",
    "    total = value_frequency_dict.get(0, 0) + value_frequency_dict.get(1, 0)\n",
    "    \n",
    "    # Calculate class weights and store them in the dictionary\n",
    "    class_weight_dic[i] = {0: (value_frequency_dict.get(1, 0) / total), 1: (value_frequency_dict.get(0, 0) / total)}\n",
    "\n",
    "# Display the computed class weights\n",
    "print(class_weight_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                3584      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,650\n",
      "Trainable params: 3,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a Sequential model\n",
    "model = Sequential([\n",
    "    Dense(units=32, input_shape=(num_col_x,), activation='sigmoid'),\n",
    "    # Additional layers (commented out for simplicity)\n",
    "    # Dense(units=64, activation='relu'), \n",
    "    # Dense(units=128, activation='relu'), \n",
    "    # Dense(units=32, activation='relu'),\n",
    "    # Dense(units=16, activation='sigmoid'), \n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with specified optimizer, loss function, and metrics\n",
    "model.compile(optimizer=Adam(learning_rate=LearningRate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train Model\n",
    "There will be a model for each medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medicine 1 of 10: 桂枝\n",
      "Training stopped at epoch 478\n",
      "20/20 [==============================] - 0s 435us/step\n",
      "5/5 [==============================] - 0s 500us/step\n",
      "Processing medicine 2 of 10: 柴胡\n",
      "Training stopped at epoch 405\n",
      "20/20 [==============================] - 0s 554us/step\n",
      "5/5 [==============================] - 0s 500us/step\n",
      "Processing medicine 3 of 10: 黃芩\n",
      "Training stopped at epoch 293\n",
      "20/20 [==============================] - 0s 421us/step\n",
      "5/5 [==============================] - 0s 750us/step\n",
      "Processing medicine 4 of 10: 茯苓\n",
      "Training stopped at epoch 287\n",
      "20/20 [==============================] - 0s 686us/step\n",
      "5/5 [==============================] - 0s 750us/step\n",
      "Processing medicine 5 of 10: 澤瀉\n",
      "Training stopped at epoch 209\n",
      "20/20 [==============================] - 0s 606us/step\n",
      "5/5 [==============================] - 0s 747us/step\n",
      "Processing medicine 6 of 10: 附子\n",
      "Training stopped at epoch 353\n",
      "20/20 [==============================] - 0s 396us/step\n",
      "5/5 [==============================] - 0s 500us/step\n",
      "Processing medicine 7 of 10: 甘草\n",
      "Training stopped at epoch 348\n",
      "20/20 [==============================] - 0s 368us/step\n",
      "5/5 [==============================] - 0s 750us/step\n",
      "Processing medicine 8 of 10: 當歸\n",
      "Training stopped at epoch 251\n",
      "20/20 [==============================] - 0s 500us/step\n",
      "5/5 [==============================] - 0s 626us/step\n",
      "Processing medicine 9 of 10: 白芍\n",
      "Training stopped at epoch 281\n",
      "20/20 [==============================] - 0s 606us/step\n",
      "5/5 [==============================] - 0s 500us/step\n",
      "Processing medicine 10 of 10: 炙甘草\n",
      "Training stopped at epoch 273\n",
      "20/20 [==============================] - 0s 554us/step\n",
      "5/5 [==============================] - 0s 631us/step\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize dictionaries to store results and training history\n",
    "result_df_dict = {}        # Dictionary of DataFrames\n",
    "accuracy_dict = {}         # Dictionary of accuracy for each medicine\n",
    "prediction_train_dict = {}  # Dictionary of raw predictions for the training set\n",
    "prediction_val_dict = {}    # Dictionary of raw predictions for the validation set\n",
    "\n",
    "# Iterate over each medicine\n",
    "for i in range(train_y.shape[1]):\n",
    "    chosen_col = train_y.iloc[:, i].copy()\n",
    "    \n",
    "    # Ensure that the chosen column is a pandas Series\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(train_y))\n",
    "    \n",
    "    print(f\"Processing medicine {i + 1} of {train_y.shape[1]}: {chosen_col.name}\")\n",
    "\n",
    "    # Convert the chosen column to NumPy array\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "\n",
    "    # Copy the corresponding validation set column\n",
    "    y_val_chosen_col = val_y.iloc[:, i].copy()\n",
    "    \n",
    "    # Ensure that the validation set column is a pandas Series\n",
    "    assert(isinstance(y_val_chosen_col, pd.Series))\n",
    "    assert(len(y_val_chosen_col) == len(val_y))\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model for the current medicine\n",
    "    Model = model.fit(\n",
    "        x=X_np,\n",
    "        y=chosen_y_np,\n",
    "        class_weight=class_weight_dic[i] if UseClassWeight else None,\n",
    "        epochs=5000,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Print when training stopped\n",
    "    print(f\"Training stopped at epoch {Model.epoch[-1]}\")\n",
    "    \n",
    "    # Predict against the training set for diagnosing overfitting or underfitting\n",
    "    predictions_train_set = model.predict(X_np)\n",
    "    \n",
    "    # Save raw result numpy array of training set to the dictionary\n",
    "    prediction_train_dict[chosen_col.name] = predictions_train_set\n",
    "    \n",
    "    # Make predictions for the validation set\n",
    "    predictions_val_set = model.predict(X_val_np)\n",
    "    \n",
    "    # Save raw result numpy array of validation set to the dictionary\n",
    "    prediction_val_dict[chosen_col.name] = predictions_val_set\n",
    "    \n",
    "    # Plotting loss vs. epoch\n",
    "    # plt.plot(Model.history['loss'], label='Training Loss')\n",
    "    # plt.title('Loss vs. Epoch')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Handle result\n",
    "\n",
    "### 7.1 Calculate the f1 score of the training dataset and store the values in TrainMedicineDictioanry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainMedicineDictioanry:\n",
      "桂枝 {'TP': 262, 'FP': 6, 'FN': 9, 'TN': 360, 'precision': 0.9776119402985075, 'recall': 0.966789667896679, 'f1-score': 0.9721706864564007}\n",
      "柴胡 {'TP': 202, 'FP': 6, 'FN': 3, 'TN': 426, 'precision': 0.9711538461538461, 'recall': 0.9853658536585366, 'f1-score': 0.9782082324455206}\n",
      "黃芩 {'TP': 250, 'FP': 2, 'FN': 3, 'TN': 382, 'precision': 0.9920634920634921, 'recall': 0.9881422924901185, 'f1-score': 0.9900990099009901}\n",
      "茯苓 {'TP': 201, 'FP': 6, 'FN': 7, 'TN': 423, 'precision': 0.9710144927536232, 'recall': 0.9663461538461539, 'f1-score': 0.9686746987951806}\n",
      "澤瀉 {'TP': 218, 'FP': 2, 'FN': 8, 'TN': 409, 'precision': 0.990909090909091, 'recall': 0.9646017699115044, 'f1-score': 0.9775784753363229}\n",
      "附子 {'TP': 262, 'FP': 7, 'FN': 6, 'TN': 362, 'precision': 0.9739776951672863, 'recall': 0.9776119402985075, 'f1-score': 0.9757914338919925}\n",
      "甘草 {'TP': 340, 'FP': 1, 'FN': 21, 'TN': 275, 'precision': 0.9970674486803519, 'recall': 0.9418282548476454, 'f1-score': 0.9686609686609687}\n",
      "當歸 {'TP': 208, 'FP': 1, 'FN': 3, 'TN': 425, 'precision': 0.9952153110047847, 'recall': 0.985781990521327, 'f1-score': 0.9904761904761905}\n",
      "白芍 {'TP': 313, 'FP': 2, 'FN': 13, 'TN': 309, 'precision': 0.9936507936507937, 'recall': 0.9601226993865031, 'f1-score': 0.9765990639625585}\n",
      "炙甘草 {'TP': 359, 'FP': 1, 'FN': 22, 'TN': 255, 'precision': 0.9972222222222222, 'recall': 0.9422572178477691, 'f1-score': 0.9689608636977058}\n",
      "overall {'TP': 2615, 'FP': 34, 'FN': 95, 'TN': 3626, 'precision': 0.9871649679124198, 'recall': 0.9649446494464945, 'f1-score': 0.9759283448404553}\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) for the training set\n",
    "total_tp_train = 0\n",
    "total_fp_train = 0\n",
    "total_tn_train = 0\n",
    "total_fn_train = 0\n",
    "\n",
    "# Create a dictioanry to store all values of a medicine\n",
    "TrainMedicineDictioanry = {}\n",
    "\n",
    "# Iterate through each medicine's raw prediction array\n",
    "for key, arr in prediction_train_dict.items():\n",
    "    \n",
    "    # Create a DataFrame from the raw prediction array\n",
    "    df_tmp = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "\n",
    "    # Determine the predicted value based on probabilities\n",
    "    df_tmp[\"predicted value\"] = np.where(df_tmp[\"predicted as 0\"] > df_tmp[\"predicted as 1\"], 0, 1)\n",
    "    \n",
    "    # Get the column number of the current medicine in the training labels\n",
    "    col_num = train_y.columns.get_loc(key)\n",
    "    \n",
    "    # Add ground truth values to the DataFrame\n",
    "    df_tmp[\"ground truth\"] = train_y.iloc[:, col_num].copy().values\n",
    "    \n",
    "    TP = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FP = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FN = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    TN = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate TP, FP, FN, TN for the current medicine\n",
    "    total_tp_train += TP\n",
    "    total_fp_train += FP\n",
    "    total_fn_train += FN\n",
    "    total_tn_train += TN\n",
    "    \n",
    "    TrainMedicineDictioanry[key] = {\n",
    "        \"TP\" : TP,\n",
    "        \"FP\" : FP,\n",
    "        \"FN\" : FN,\n",
    "        \"TN\" : TN,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : f1score\n",
    "    }\n",
    "\n",
    "precision = total_tp_train / (total_tp_train + total_fp_train)\n",
    "recall = total_tp_train / (total_tp_train + total_fn_train)\n",
    "\n",
    "TrainMedicineDictioanry[\"overall\"] = {\n",
    "        \"TP\" : total_tp_train,\n",
    "        \"FP\" : total_fp_train,\n",
    "        \"FN\" : total_fn_train,\n",
    "        \"TN\" : total_tn_train,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"TrainMedicineDictioanry:\")\n",
    "for key in TrainMedicineDictioanry:\n",
    "    print(key, TrainMedicineDictioanry[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Calculate the f1 score of the validation dataset and store the values in ValMedicineDictioanry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValMedicineDictioanry:\n",
      "桂枝 {'TP': 32, 'FP': 40, 'FN': 34, 'TN': 53, 'precision': 0.4444444444444444, 'recall': 0.48484848484848486, 'f1-score': 0.463768115942029}\n",
      "柴胡 {'TP': 21, 'FP': 21, 'FN': 35, 'TN': 82, 'precision': 0.5, 'recall': 0.375, 'f1-score': 0.42857142857142855}\n",
      "黃芩 {'TP': 31, 'FP': 25, 'FN': 36, 'TN': 67, 'precision': 0.5535714285714286, 'recall': 0.4626865671641791, 'f1-score': 0.5040650406504066}\n",
      "茯苓 {'TP': 23, 'FP': 25, 'FN': 38, 'TN': 73, 'precision': 0.4791666666666667, 'recall': 0.3770491803278688, 'f1-score': 0.4220183486238532}\n",
      "澤瀉 {'TP': 32, 'FP': 19, 'FN': 40, 'TN': 68, 'precision': 0.6274509803921569, 'recall': 0.4444444444444444, 'f1-score': 0.5203252032520325}\n",
      "附子 {'TP': 43, 'FP': 29, 'FN': 35, 'TN': 52, 'precision': 0.5972222222222222, 'recall': 0.5512820512820513, 'f1-score': 0.5733333333333334}\n",
      "甘草 {'TP': 54, 'FP': 41, 'FN': 35, 'TN': 29, 'precision': 0.5684210526315789, 'recall': 0.6067415730337079, 'f1-score': 0.5869565217391305}\n",
      "當歸 {'TP': 22, 'FP': 40, 'FN': 30, 'TN': 67, 'precision': 0.3548387096774194, 'recall': 0.4230769230769231, 'f1-score': 0.3859649122807018}\n",
      "白芍 {'TP': 53, 'FP': 29, 'FN': 43, 'TN': 34, 'precision': 0.6463414634146342, 'recall': 0.5520833333333334, 'f1-score': 0.5955056179775281}\n",
      "炙甘草 {'TP': 54, 'FP': 37, 'FN': 39, 'TN': 29, 'precision': 0.5934065934065934, 'recall': 0.5806451612903226, 'f1-score': 0.5869565217391305}\n",
      "overall {'TP': 365, 'FP': 306, 'FN': 365, 'TN': 554, 'precision': 0.5439642324888226, 'recall': 0.5, 'f1-score': 0.5210563882940756}\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) for the training set\n",
    "total_tp_train = 0\n",
    "total_fp_train = 0\n",
    "total_tn_train = 0\n",
    "total_fn_train = 0\n",
    "\n",
    "# Create a dictioanry to store all values of a medicine\n",
    "ValMedicineDictioanry = {}\n",
    "\n",
    "# Iterate through each medicine's raw prediction array\n",
    "for key, arr in prediction_val_dict.items():\n",
    "    \n",
    "    # Create a DataFrame from the raw prediction array\n",
    "    df_tmp = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "\n",
    "    # Determine the predicted value based on probabilities\n",
    "    df_tmp[\"predicted value\"] = np.where(df_tmp[\"predicted as 0\"] > df_tmp[\"predicted as 1\"], 0, 1)\n",
    "    \n",
    "    # Get the column number of the current medicine in the training labels\n",
    "    col_num = val_y.columns.get_loc(key)\n",
    "    \n",
    "    # Add ground truth values to the DataFrame\n",
    "    df_tmp[\"ground truth\"] = val_y.iloc[:, col_num].copy().values\n",
    "    \n",
    "    TP = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FP = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FN = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    TN = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate TP, FP, FN, TN for the current medicine\n",
    "    total_tp_train += TP\n",
    "    total_fp_train += FP\n",
    "    total_fn_train += FN\n",
    "    total_tn_train += TN\n",
    "    \n",
    "    ValMedicineDictioanry[key] = {\n",
    "        \"TP\" : TP,\n",
    "        \"FP\" : FP,\n",
    "        \"FN\" : FN,\n",
    "        \"TN\" : TN,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : f1score\n",
    "    }\n",
    "\n",
    "precision = total_tp_train / (total_tp_train + total_fp_train)\n",
    "recall = total_tp_train / (total_tp_train + total_fn_train)\n",
    "\n",
    "ValMedicineDictioanry[\"overall\"] = {\n",
    "        \"TP\" : total_tp_train,\n",
    "        \"FP\" : total_fp_train,\n",
    "        \"FN\" : total_fn_train,\n",
    "        \"TN\" : total_tn_train,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"ValMedicineDictioanry:\")\n",
    "for key in ValMedicineDictioanry:\n",
    "    print(key, ValMedicineDictioanry[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following were all commented since i have no idead what this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_path_suffix = \"each_med_csv\"   # type the dir for you to remember where u save the result\n",
    "# for key, df in result_df_dict.items():\n",
    "#     print(f\"DataFrame for {key}:\")\n",
    "#     #myutil.print_df(df)\n",
    "#     myutil.df_to_csv(df, save_path=(\"./result/\"+file_path_suffix), file_prefix=key)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make df for all f1_score of each medicine\n",
    "# all_f1_df = pd.DataFrame([(key, val['f1_score'], (val['TP']+val['FN'])) for key, val in acc_each_med.items()], columns=['medicine', 'f1_score', 'TP+FN'])\n",
    "# myutil.df_to_csv(all_f1_df, save_path=(\"./result/\"+file_path_suffix), file_prefix='all_f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save f1_score and TP/FP/TN/TN/FN\n",
    "\n",
    "# spec_str = \"model layer:  32-64-128-64-32 units, activation: relu, optimizer: Adam, learning rate: 0.001, epochs: 1000, batch_size: 32, num_med: all. del_med_under_thres: 0\"\n",
    "# # need to type this spec str each time to record the result\n",
    "\n",
    "# file_path=\"./result/1_med_accuracy\"\n",
    "# myutil.dict_to_txt(acc_each_med, save_path=file_path, \n",
    "#                    file_prefix=\"accuracy_each_med\",\n",
    "#                    textbox=spec_str)\n",
    "\n",
    "\n",
    "# myutil.dict_to_txt(train_set_acc, save_path=file_path, \n",
    "#                    file_prefix=\"accuracy_train_set\",\n",
    "#                    textbox=\"train set\"+spec_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
