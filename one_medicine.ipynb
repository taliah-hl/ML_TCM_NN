{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the notebook, a model is trained for each medicine\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "the last few blocks were commmented since I have no clue what they were for.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the entire column related to a specific medicine if its occurrence is below the defined threshold.\n",
    "# In the given dataset, setting this threshold to 250 would retain columns for only the top 10 most frequently used medicines.\n",
    "DeleteMedThreshold = 250\n",
    "\n",
    "# Determine the number of medicines to be trained as output\n",
    "# The total number of medicine is 102\n",
    "NumMedTrain = 102\n",
    "\n",
    "# Decide whether to enhance accuracy by utilizing class weights\n",
    "UseClassWeight = True\n",
    "\n",
    "# Decide learning_rate\n",
    "LearningRate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing TensorFlow for deep learning\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "# Importing scikit-learn for data preprocessing and utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Importing custom utility functions\n",
    "from utility_file import my_utilities as myutil\n",
    "from utility_file import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read Data\n",
    "Load the data using the custom module \"load_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "ReadData:\n",
      "Type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n",
      "End of ReadData\n",
      "--------------------------------------------------------------------------------\n",
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols).\n",
      "Shape of y = (796 rows, 102 cols).\n",
      "End of SplitXY\n",
      "--------------------------------------------------------------------------------\n",
      "In load_data_for_1_med_with_debug of load_data.py, random_seed= 1\n",
      "After SplitXY, total number of 0, 1 in y:\n",
      "Number of 0s: 72318\n",
      "Number of 1s: 8874\n",
      "--------------------------------------------------------------------------------\n",
      "DeleteMedicine: shape of y is (796, 10).\n",
      "save med num done\n",
      "Train_X.shape:  (637, 111)\n",
      "Train_y.shape:  (637, 10)\n",
      "\n",
      "Split Training Validation\n",
      "Number of 0s in train_y: 3660\n",
      "Number of 1s train_y: 2710\n",
      "Number of 0s in val_y: 860\n",
      "Number of 1s val_y: 730\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load data for training a model with one specific medicine, including debugging information\n",
    "(X_np, X_val_np, train_y, val_y, \n",
    " num_col_x, num_1_valy, num_0_valy) = load_data.load_data_for_1_med_with_debug(del_med_thres=DeleteMedThreshold, random_seed=1, n=NumMedTrain)\n",
    "\n",
    "# Ensure the correct data types for loaded variables\n",
    "assert isinstance(X_np, np.ndarray)\n",
    "assert isinstance(X_val_np, np.ndarray)\n",
    "assert isinstance(train_y, pd.DataFrame)\n",
    "assert isinstance(val_y, pd.DataFrame)\n",
    "assert isinstance(num_col_x, int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Type Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in y: 0\n",
      "Number of str values in y: 0\n",
      "Number of int values in y: 1590\n",
      "Number of float values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below to display the DataFrame content and structure\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "\n",
    "# Uncomment the line below to print the DataFrame directly\n",
    "# print(val_y)\n",
    "\n",
    "# Checking:\n",
    "\n",
    "# Counting NA values in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "\n",
    "# Counting str values in y\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "\n",
    "# Counting int values in y\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "\n",
    "# Counting float values in y\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0.42543171114599687, 1: 0.5745682888540031}, 1: {0: 0.3218210361067504, 1: 0.6781789638932496}, 2: {0: 0.39717425431711145, 1: 0.6028257456828885}, 3: {0: 0.32653061224489793, 1: 0.673469387755102}, 4: {0: 0.35478806907378335, 1: 0.6452119309262166}, 5: {0: 0.4207221350078493, 1: 0.5792778649921507}, 6: {0: 0.5667189952904239, 1: 0.43328100470957615}, 7: {0: 0.33124018838304553, 1: 0.6687598116169545}, 8: {0: 0.5117739403453689, 1: 0.48822605965463106}, 9: {0: 0.598116169544741, 1: 0.40188383045525905}}\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'train_y' DataFrame to a NumPy array\n",
    "train_y_np = np.array(train_y)\n",
    "\n",
    "# Determine the number of labels (columns) in the array\n",
    "num_labels = train_y_np.shape[1]\n",
    "\n",
    "# Initialize an empty dictionary to store class weights for each label\n",
    "class_weight_dic = {}\n",
    "\n",
    "# Iterate over each label column\n",
    "for i in range(num_labels):\n",
    "    # Count the occurrences of each class (0 and 1) in the current label column\n",
    "    unique_values, counts = np.unique(train_y_np[:, i], return_counts=True)\n",
    "    \n",
    "    # Create a dictionary mapping class values to their frequencies\n",
    "    value_frequency_dict = dict(zip(unique_values, counts))\n",
    "    \n",
    "    # Calculate the total number of occurrences for normalization\n",
    "    total = value_frequency_dict.get(0, 0) + value_frequency_dict.get(1, 0)\n",
    "    \n",
    "    # Calculate class weights and store them in the dictionary\n",
    "    class_weight_dic[i] = {0: (value_frequency_dict.get(1, 0) / total), 1: (value_frequency_dict.get(0, 0) / total)}\n",
    "\n",
    "# Display the computed class weights\n",
    "print(class_weight_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 64)                7168      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,810\n",
      "Trainable params: 9,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a Sequential model\n",
    "model = Sequential([\n",
    "    Dense(units=64, input_shape=(num_col_x,), activation='sigmoid'),\n",
    "    # Additional layers (commented out for simplicity)\n",
    "    # Dense(units=64, activation='relu'), \n",
    "    # Dense(units=128, activation='relu'), \n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=16, activation='sigmoid'), \n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Compile the model with specified optimizer, loss function, and metrics\n",
    "model.compile(optimizer=Adam(learning_rate=LearningRate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train Model\n",
    "There will be a model for each medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medicine 1 of 10: 桂枝\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stopped at epoch 366\n",
      "20/20 [==============================] - 0s 825us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 2 of 10: 柴胡\n",
      "Training stopped at epoch 316\n",
      "20/20 [==============================] - 0s 793us/step\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "Processing medicine 3 of 10: 黃芩\n",
      "Training stopped at epoch 214\n",
      "20/20 [==============================] - 0s 738us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 4 of 10: 茯苓\n",
      "Training stopped at epoch 58\n",
      "20/20 [==============================] - 0s 774us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 5 of 10: 澤瀉\n",
      "Training stopped at epoch 64\n",
      "20/20 [==============================] - 0s 741us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 6 of 10: 附子\n",
      "Training stopped at epoch 54\n",
      "20/20 [==============================] - 0s 662us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 7 of 10: 甘草\n",
      "Training stopped at epoch 62\n",
      "20/20 [==============================] - 0s 714us/step\n",
      "5/5 [==============================] - 0s 500us/step\n",
      "Processing medicine 8 of 10: 當歸\n",
      "Training stopped at epoch 64\n",
      "20/20 [==============================] - 0s 766us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 9 of 10: 白芍\n",
      "Training stopped at epoch 647\n",
      "20/20 [==============================] - 0s 694us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Processing medicine 10 of 10: 炙甘草\n",
      "Training stopped at epoch 588\n",
      "20/20 [==============================] - 0s 643us/step\n",
      "5/5 [==============================] - 0s 750us/step\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize dictionaries to store results and training history\n",
    "result_df_dict = {}        # Dictionary of DataFrames of each medicine in training set \n",
    "accuracy_dict = {}         # Dictionary of accuracy for each medicine\n",
    "prediction_train_dict = {}  # Dictionary of raw predictions for the training set\n",
    "prediction_val_dict = {}    # Dictionary of raw predictions for the validation set\n",
    "\n",
    "# Iterate over each medicine\n",
    "for i in range(train_y.shape[1]):\n",
    "    chosen_col = train_y.iloc[:, i].copy()\n",
    "    \n",
    "    # Ensure that the chosen column is a pandas Series\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(train_y))\n",
    "    \n",
    "    print(f\"Processing medicine {i + 1} of {train_y.shape[1]}: {chosen_col.name}\")\n",
    "\n",
    "    # Convert the chosen column to NumPy array\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "\n",
    "    # Copy the corresponding validation set column\n",
    "    y_val_chosen_col = val_y.iloc[:, i].copy()\n",
    "    \n",
    "    # Ensure that the validation set column is a pandas Series\n",
    "    assert(isinstance(y_val_chosen_col, pd.Series))\n",
    "    assert(len(y_val_chosen_col) == len(val_y))\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model for the current medicine\n",
    "    Model = model.fit(\n",
    "        x=X_np,\n",
    "        y=chosen_y_np,\n",
    "        class_weight=class_weight_dic[i] if UseClassWeight else None,\n",
    "        epochs=5000,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Print when training stopped\n",
    "    print(f\"Training stopped at epoch {Model.epoch[-1]}\")\n",
    "    \n",
    "    # Predict against the training set for diagnosing overfitting or underfitting\n",
    "    predictions_train_set = model.predict(X_np)\n",
    "    \n",
    "    # Save raw result numpy array of training set to the dictionary\n",
    "    prediction_train_dict[chosen_col.name] = predictions_train_set\n",
    "    \n",
    "    # Make predictions for the validation set\n",
    "    predictions_val_set = model.predict(X_val_np)\n",
    "    \n",
    "    # Save raw result numpy array of validation set to the dictionary\n",
    "    prediction_val_dict[chosen_col.name] = predictions_val_set\n",
    "    \n",
    "    # Plotting loss vs. epoch\n",
    "    # plt.plot(Model.history['loss'], label='Training Loss')\n",
    "    # plt.title('Loss vs. Epoch')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Handle result\n",
    "\n",
    "### 7.1 Calculate the f1 score of the training dataset and store the values in TrainMedicineDictioanry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainMedicineDictioanry:\n",
      "overall {'TP': 2402, 'FP': 1534, 'FN': 308, 'TN': 2126, 'precision': 0.6102642276422764, 'recall': 0.8863468634686347, 'f1-score': 0.7228408065001505}\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) for the training set\n",
    "total_tp_train = 0\n",
    "total_fp_train = 0\n",
    "total_tn_train = 0\n",
    "total_fn_train = 0\n",
    "\n",
    "# Create a dictioanry to store all values of a medicine\n",
    "TrainMedicineDictioanry = {}\n",
    "\n",
    "# Iterate through each medicine's raw prediction array\n",
    "for key, arr in prediction_train_dict.items():\n",
    "    \n",
    "    # Create a DataFrame from the raw prediction array\n",
    "    df_tmp = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "\n",
    "    # Determine the predicted value based on probabilities\n",
    "    df_tmp[\"predicted value\"] = np.where(df_tmp[\"predicted as 0\"] > df_tmp[\"predicted as 1\"], 0, 1)\n",
    "    \n",
    "    # Get the column number of the current medicine in the training labels\n",
    "    col_num = train_y.columns.get_loc(key)\n",
    "    \n",
    "    # Add ground truth values to the DataFrame\n",
    "    df_tmp[\"ground truth\"] = train_y.iloc[:, col_num].copy().values\n",
    "    \n",
    "    \n",
    "    TP = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FP = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FN = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    TN = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate TP, FP, FN, TN for the current medicine\n",
    "    total_tp_train += TP\n",
    "    total_fp_train += FP\n",
    "    total_fn_train += FN\n",
    "    total_tn_train += TN\n",
    "    \n",
    "    TrainMedicineDictioanry[key] = {\n",
    "        \"TP\" : TP,\n",
    "        \"FP\" : FP,\n",
    "        \"FN\" : FN,\n",
    "        \"TN\" : TN,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : f1score\n",
    "    }\n",
    "\n",
    "precision = total_tp_train / (total_tp_train + total_fp_train) if (total_tp_train + total_fp_train) > 0 else 0\n",
    "recall = total_tp_train / (total_tp_train + total_fn_train) if (total_tp_train + total_fn_train) > 0 else 0\n",
    "\n",
    "TrainMedicineDictioanry[\"overall\"] = {\n",
    "        \"TP\" : total_tp_train,\n",
    "        \"FP\" : total_fp_train,\n",
    "        \"FN\" : total_fn_train,\n",
    "        \"TN\" : total_tn_train,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"TrainMedicineDictioanry:\")\n",
    "# for key in TrainMedicineDictioanry:\n",
    "#     print(key, TrainMedicineDictioanry[key])\n",
    "print(\"overall\", TrainMedicineDictioanry[\"overall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Calculate the f1 score of the validation dataset and store the values in ValMedicineDictioanry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValMedicineDictioanry:\n",
      "overall {'TP': 487, 'FP': 475, 'FN': 243, 'TN': 385, 'precision': 0.5062370062370062, 'recall': 0.6671232876712329, 'f1-score': 0.5756501182033097}\n"
     ]
    }
   ],
   "source": [
    "# Calculate True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) for the training set\n",
    "total_tp_train = 0\n",
    "total_fp_train = 0\n",
    "total_tn_train = 0\n",
    "total_fn_train = 0\n",
    "\n",
    "# Create a dictioanry to store all values of a medicine\n",
    "ValMedicineDictioanry = {}\n",
    "\n",
    "# Iterate through each medicine's raw prediction array\n",
    "for key, arr in prediction_val_dict.items():\n",
    "    \n",
    "    # Create a DataFrame from the raw prediction array\n",
    "    df_tmp = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "\n",
    "    # Determine the predicted value based on probabilities\n",
    "    df_tmp[\"predicted value\"] = np.where(df_tmp[\"predicted as 0\"] > df_tmp[\"predicted as 1\"], 0, 1)\n",
    "    \n",
    "    # Get the column number of the current medicine in the training labels\n",
    "    col_num = val_y.columns.get_loc(key)\n",
    "    \n",
    "    # Add ground truth values to the DataFrame\n",
    "    df_tmp[\"ground truth\"] = val_y.iloc[:, col_num].copy().values\n",
    "    result_df_dict[key] = df_tmp\n",
    "    \n",
    "    TP = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FP = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    FN = ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    TN = ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate TP, FP, FN, TN for the current medicine\n",
    "    total_tp_train += TP\n",
    "    total_fp_train += FP\n",
    "    total_fn_train += FN\n",
    "    total_tn_train += TN\n",
    "    \n",
    "    ValMedicineDictioanry[key] = {\n",
    "        \"TP\" : TP,\n",
    "        \"FP\" : FP,\n",
    "        \"FN\" : FN,\n",
    "        \"TN\" : TN,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : f1score\n",
    "    }\n",
    "\n",
    "precision = total_tp_train / (total_tp_train + total_fp_train) if (total_tp_train + total_fp_train) > 0 else 0\n",
    "recall = total_tp_train / (total_tp_train + total_fn_train) if (total_tp_train + total_fn_train) > 0 else 0\n",
    "\n",
    "ValMedicineDictioanry[\"overall\"] = {\n",
    "        \"TP\" : total_tp_train,\n",
    "        \"FP\" : total_fp_train,\n",
    "        \"FN\" : total_fn_train,\n",
    "        \"TN\" : total_tn_train,\n",
    "        \"precision\" : precision,\n",
    "        \"recall\" : recall,\n",
    "        \"f1-score\" : 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"ValMedicineDictioanry:\")\n",
    "# for key in ValMedicineDictioanry:\n",
    "#     print(key, ValMedicineDictioanry[key])\n",
    "print(\"overall\", ValMedicineDictioanry[\"overall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Result Saving\n",
    "\n",
    "Saving resulting DataFrames to csv and dictionaries to txt for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桂枝 saved to ./result/one_medicine_result/桂枝_5.csv\n",
      "柴胡 saved to ./result/one_medicine_result/柴胡_5.csv\n",
      "黃芩 saved to ./result/one_medicine_result/黃芩_5.csv\n",
      "茯苓 saved to ./result/one_medicine_result/茯苓_5.csv\n",
      "澤瀉 saved to ./result/one_medicine_result/澤瀉_5.csv\n",
      "附子 saved to ./result/one_medicine_result/附子_5.csv\n",
      "甘草 saved to ./result/one_medicine_result/甘草_5.csv\n",
      "當歸 saved to ./result/one_medicine_result/當歸_5.csv\n",
      "白芍 saved to ./result/one_medicine_result/白芍_5.csv\n",
      "炙甘草 saved to ./result/one_medicine_result/炙甘草_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Path identifier for saving results in the directory\n",
    "file_path = \"./result/one_medicine_result\"\n",
    "\n",
    "# Exporting the result of each medicine in validation set to csv file\n",
    "for key, df in result_df_dict.items():\n",
    "\n",
    "    # Uncomment if you want to print the DataFrames on console\n",
    "    # print(f\"DataFrame for {key}:\")\n",
    "    # myutil.print_df(df)\n",
    "    myutil.df_to_csv(df, save_path=file_path, file_prefix=key)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score_all_medicine_val saved to ./result/result_all_medicine_val/f1_score_all_medicine_val_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to record f1 score, TP/FP/TN/FN of each medicine\n",
    "all_f1_df = pd.DataFrame([(key, val['f1-score'], val['precision'], val['recall'], \n",
    "                           val['TP'], val['FP'], val['TN'], val['FN']) for key, val in ValMedicineDictioanry.items()], \n",
    "                         columns=['medicine', 'f1-score','precision', 'recall', 'TP', 'FP', 'TN', 'FN']\n",
    "                        )\n",
    "\n",
    "file_path = \"./result/result_all_medicine_val\"\n",
    "\n",
    "# Exporting the DataFrame to csv file\n",
    "myutil.df_to_csv(all_f1_df, save_path=file_path, file_prefix='f1_score_all_medicine_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValMedicineDict saved to ./result/one_med_ValMedicineDict/ValMedicineDict_5.txt\n",
      "TrainMedicineDict saved to ./result/one_med_TrainMedicineDict/TrainMedicineDict_5.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specification string to be printed onto the resulting txt files\n",
    "# This is for recording the training specifications that produced these results\n",
    "training_specification  = \"model layer:  32-64-128-64-32 units, activation: relu, optimizer: Adam, learning rate: 0.001, epochs: 1000, batch_size: 32, num_med: all. del_med_under_thres: 0\"\n",
    "\n",
    "file_path=\"./result/one_med_ValMedicineDict\"\n",
    "\n",
    "# Exporting f1-score, TP/FP/TN/FN of the training and validation sets to text files\n",
    "myutil.dict_to_txt(ValMedicineDictioanry, save_path=file_path, \n",
    "                   file_prefix=\"ValMedicineDict\",\n",
    "                   textbox=training_specification )\n",
    "\n",
    "file_path=\"./result/one_med_TrainMedicineDict\"\n",
    "myutil.dict_to_txt(TrainMedicineDictioanry, save_path=file_path, \n",
    "                   file_prefix=\"TrainMedicineDict\",\n",
    "                   textbox=\"train set, \"+training_specification )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
