{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use simplified data 2\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data 處理:\n",
    "\n",
    "data v2 刪掉所有文字input\n",
    "\n",
    "deleted 回診,西藥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(FILENAME):\n",
    "    data = pd.read_csv(FILENAME, encoding='ANSI')\n",
    "    print(\"in ReadData\")\n",
    "    print(\"type of data:\", type(data))\n",
    "    print(f'Shape of data = ({data.shape[0]} rows, {data.shape[1]} cols).')\n",
    "    return data\n",
    "\n",
    "def SplitXY(data, data_left_bound, label_left_bound, data_right_bound=None, label_right_bound=None):\n",
    "    \"\"\"label_left_bound: 第一個藥材的col no.\"\"\"\n",
    "    # Body status: 1~3, Diagnosis: 4~7, Symptom: 11~124\n",
    "    # Prescription: 125~226\n",
    "    # split_X = list(range(0, label_left_bound))\n",
    "    # split_Y = list(range(label_left_bound, len(data.columns)))\n",
    "    if label_right_bound is None:\n",
    "        label_right_bound = len(data.columns)\n",
    "    if data_right_bound is None:\n",
    "        data_right_bound = label_left_bound\n",
    "    X = data.iloc[1:, list(range(data_left_bound, data_right_bound))]\n",
    "    y = data.iloc[1:, list(range(label_left_bound, label_right_bound))]\n",
    "    \n",
    "    # Debug\n",
    "    print(\"SplitXY:\")\n",
    "    print(f'Shape of X = ({X.shape[0]} rows, {X.shape[1]} cols). First 10 data of X:')\n",
    "    print(X.iloc[:10, :10])\n",
    "    print(f'Shape of y = ({y.shape[0]} rows, {y.shape[1]} cols). First 10 data of y:')\n",
    "    print(y.iloc[:10, :10])\n",
    "    return X, y\n",
    "\n",
    "def SplitNparr(original_arr: np.ndarray, train_portion: float)->tuple:\n",
    "    data_len =len(original_arr)\n",
    "    train_len = int(data_len * train_portion)\n",
    "    indices = list(range(data_len))\n",
    "    random.shuffle(indices)\n",
    "    train_idx = indices[:train_len]\n",
    "    validate_idx = indices[train_len:]\n",
    "    training_arr  = original_arr[train_idx]\n",
    "    validation_arr = original_arr[validate_idx]\n",
    "\n",
    "    return (training_arr,validation_arr)\n",
    "\n",
    "\n",
    "def Split1Df(data: pd.DataFrame,  train_size: float, random_state: int=None):\n",
    "    \"\"\"\n",
    "    Split the data into training and validation sets.\n",
    "\n",
    "    Param:\n",
    "    - data: any data\n",
    "    - train_size: Proportion of in train set.\n",
    "    - random_state: Seed \n",
    "\n",
    "    return:\n",
    "    - data_train: data for the training set.\n",
    "    - data_val: data for the validation set.\n",
    "\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "    data_len = len(data)\n",
    "    val_len = int(data_len * (1-train_size))\n",
    "    indices = list(range(data_len))\n",
    "    random.shuffle(indices)\n",
    "    val_idx = indices[:val_len]\n",
    "    train_idx = indices[val_len:]\n",
    "\n",
    "\n",
    "    data_train, data_val = data.loc[train_idx], data.loc[val_idx]\n",
    "   \n",
    "    return data_train, data_val\n",
    "\n",
    "def SplitBothXy_Df(X: pd.DataFrame, y: pd.DataFrame, train_size: float, random_state: int=None):\n",
    "    \"\"\"\n",
    "    Split the data into training and validation sets.\n",
    "\n",
    "    Param:\n",
    "    - X: Features \n",
    "    - y: Target variable\n",
    "    - train_size: Proportion of in train set.\n",
    "    - random_state: Seed \n",
    "\n",
    "    return:\n",
    "    - X_train: Features for the training set.\n",
    "    - X_val: Features for the validation set.\n",
    "    - y_train: Target variable for the training set.\n",
    "    - y_val: Target variable for the validation set.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "    data_len = len(X)\n",
    "    val_len = int(data_len * (1-train_size))\n",
    "    indices = list(range(data_len))\n",
    "    random.shuffle(indices)\n",
    "    val_idx = indices[:val_len]\n",
    "    train_idx = indices[val_len:]\n",
    "\n",
    "\n",
    "    X_train, X_val = X.loc[train_idx], X.loc[val_idx]\n",
    "    y_train, y_val = y.loc[train_idx], y.loc[val_idx]\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ReadData\n",
      "type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n"
     ]
    }
   ],
   "source": [
    "data_pd = ReadData(\"./simplified_data/simplified_data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols). First 10 data of X:\n",
      "    肺癌  胰臟癌  肝癌  腺癌  攝護腺癌  骨癌  淋巴癌  胃癌  腦瘤  肝炎\n",
      "1    0    0   0   0     0   0    0   0   0   0\n",
      "2    0    0   0   0     0   0    0   0   0   0\n",
      "3    0    0   0   0     0   0    0   0   0   0\n",
      "4    0    0   0   0     0   0    0   0   0   0\n",
      "5    0    0   0   0     0   0    0   0   0   0\n",
      "6    0    0   0   0     0   0    0   0   0   0\n",
      "7    0    0   0   0     0   0    0   0   0   0\n",
      "8    0    0   0   0     0   0    0   0   0   0\n",
      "9    0    0   0   0     0   0    0   0   0   0\n",
      "10   0    0   0   0     0   0    0   0   0   0\n",
      "Shape of y = (796 rows, 102 cols). First 10 data of y:\n",
      "    麻黃  桂枝  荊芥  防風  細辛  白芷  生薑  辛夷  葛根  升麻\n",
      "1    0   1   0   0   1   1   0   0   0   0\n",
      "2    0   1   0   0   0   0   1   0   0   0\n",
      "3    0   0   0   0   0   0   0   0   0   0\n",
      "4    0   1   0   0   0   0   1   1   0   0\n",
      "5    0   1   0   0   0   0   0   0   0   0\n",
      "6    0   0   0   0   0   0   0   0   0   0\n",
      "7    0   1   0   0   0   0   0   0   0   0\n",
      "8    0   1   0   0   0   0   0   0   0   0\n",
      "9    0   1   0   0   1   0   0   0   0   0\n",
      "10   0   1   0   0   1   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "first_medicine_idx = 113\n",
    "\n",
    "# split data into X and y\n",
    "# x= all symptoms, diagnosis, body status\n",
    "# y= all medince\n",
    "X,y = SplitXY(data_pd, data_left_bound=2, data_right_bound=first_medicine_idx, label_left_bound=first_medicine_idx)\n",
    "\n",
    "# X is a df, split X into trainX and testX\n",
    "#train_X, val_X, train_y, val_y = SplitBothXy_Df(X, y, 0.8) # split need bug fix!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X transformed to np array\n",
      "type of X_np: <class 'numpy.ndarray'>\n",
      "shape of X_np: (796, 111)\n",
      "number of col in x: 111\n",
      "len of y: 102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_np = X.values.astype('float64')\n",
    "print(\"X transformed to np array\")\n",
    "print(\"type of X_np:\", type(X_np))\n",
    "print(\"shape of X_np:\", X_np.shape)\n",
    "num_col_x = X_np.shape[1]\n",
    "print(\"number of col in x:\",num_col_x)\n",
    "print(\"len of y:\", y.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                1792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,402\n",
      "Trainable params: 2,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build and compile model\n",
    "\n",
    "model = Sequential([\n",
    "    Dense (units=16, input_shape=(num_col_x,), activation='relu'),\n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=2, activation='softmax')\n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing medicine: 麻黃\n",
      "Epoch 1/10\n",
      "80/80 - 2s - loss: 0.4775 - accuracy: 0.8229 - 2s/epoch - 25ms/step\n",
      "Epoch 2/10\n",
      "80/80 - 0s - loss: 0.4161 - accuracy: 0.8342 - 210ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "80/80 - 0s - loss: 0.3656 - accuracy: 0.8392 - 213ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "80/80 - 0s - loss: 0.3199 - accuracy: 0.8543 - 209ms/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "80/80 - 0s - loss: 0.2494 - accuracy: 0.8869 - 212ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "80/80 - 0s - loss: 0.2053 - accuracy: 0.9083 - 217ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "80/80 - 0s - loss: 0.1774 - accuracy: 0.9384 - 212ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "80/80 - 0s - loss: 0.1464 - accuracy: 0.9485 - 222ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "80/80 - 0s - loss: 0.1094 - accuracy: 0.9598 - 222ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "80/80 - 0s - loss: 0.0886 - accuracy: 0.9686 - 214ms/epoch - 3ms/step\n",
      "processing medicine: 桂枝\n",
      "Epoch 1/10\n",
      "80/80 - 0s - loss: 0.9430 - accuracy: 0.5704 - 216ms/epoch - 3ms/step\n",
      "Epoch 2/10\n",
      "80/80 - 0s - loss: 0.6394 - accuracy: 0.6357 - 218ms/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "80/80 - 0s - loss: 0.5740 - accuracy: 0.7035 - 214ms/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "80/80 - 0s - loss: 0.4913 - accuracy: 0.7613 - 214ms/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "80/80 - 0s - loss: 0.4366 - accuracy: 0.8015 - 218ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "80/80 - 0s - loss: 0.3755 - accuracy: 0.8392 - 209ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "80/80 - 0s - loss: 0.3246 - accuracy: 0.8593 - 211ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "80/80 - 0s - loss: 0.2912 - accuracy: 0.8807 - 210ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "80/80 - 0s - loss: 0.2494 - accuracy: 0.8982 - 212ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "80/80 - 0s - loss: 0.2395 - accuracy: 0.9095 - 213ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for i in range(y.shape[1]):\n",
    "for i in range(2):\n",
    "    chosen_col = y.iloc[:,i].copy()\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(y))\n",
    "    print(\"processing medicine:\", chosen_col.name)\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "    model.fit(x=X_np, y=chosen_y_np,\n",
    "          batch_size=10, epochs=10, shuffle=True, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
