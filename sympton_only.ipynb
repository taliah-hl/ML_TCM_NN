{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data source: ./simplified_data/data_v2_grouped_symptom.csv\n",
    "\n",
    "\n",
    "\n",
    "data 處理:\n",
    "\n",
    "data v2 刪掉所有文字input, deleted 回診,西藥, 刪除非病癥/病, 人手group起相似col\n",
    "\n",
    "\n",
    "\n",
    "## train 方法:\n",
    "\n",
    "train with x that only has sympton, model layer: 16-32-32-2, epoch=100, batch_size=32,  learning rate: 0.001, activation=relu, num_med:all, del_med_under_thres=0\n",
    "\n",
    "## ready to train 但未train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import statistics\n",
    "from tabulate import tabulate\n",
    "\n",
    "# custom import \n",
    "from utility_file import my_utilities as myutil\n",
    "from utility_file import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "1. 決定是否要delete 少於某threshold的藥\n",
    "2. 用load_data裡的`load_data_for_n_med` load data\n",
    "3. 如只train 頭n 個藥-> set `only_train_1st_n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ReadData\n",
      "type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (674 rows, 150 cols).\n",
      "SplitXY:\n",
      "Shape of X = (673 rows, 46 cols). First 10 data of X:\n",
      "    乳癌  肺癌  胰臟癌  肝癌  腺癌  攝護腺癌  骨癌  淋巴癌  胃癌  腦瘤\n",
      "1    0   0    0   0   0     0   0    0   0   0\n",
      "2    0   0    0   0   0     0   0    0   0   0\n",
      "3    0   0    0   0   0     0   0    0   0   0\n",
      "4    0   1    0   0   0     0   0    0   0   0\n",
      "5    0   0    0   0   0     0   0    0   0   0\n",
      "6    0   0    0   0   0     0   0    0   0   0\n",
      "7    0   0    0   0   0     0   0    0   0   0\n",
      "8    0   0    0   0   0     0   0    0   0   0\n",
      "9    0   0    0   0   0     0   0    0   0   0\n",
      "10   1   1    0   0   0     0   1    0   0   0\n",
      "Shape of y = (673 rows, 102 cols). First 10 data of y:\n",
      "    麻黃  桂枝  荊芥  防風  細辛  白芷  生薑  辛夷  葛根  升麻\n",
      "1    0   0   0   0   0   0   0   0   0   0\n",
      "2    0   0   0   0   0   0   0   0   0   0\n",
      "3    1   0   0   0   1   0   0   0   0   0\n",
      "4    0   0   0   0   0   0   0   0   0   0\n",
      "5    0   0   0   0   0   0   0   0   0   0\n",
      "6    0   0   0   0   0   0   0   0   0   0\n",
      "7    0   0   0   0   0   0   0   0   0   0\n",
      "8    0   0   0   0   0   0   0   0   0   0\n",
      "9    0   0   0   0   0   0   0   1   0   0\n",
      "10   0   0   0   0   0   0   0   0   0   0\n",
      "=======================\n",
      "\n",
      "in load_data_for_1_med_with_debug of load_data.py, random_seed= None\n",
      "after SplitXY, total number of 0, 1 in y:\n",
      "no. of 1: 8751\n",
      "no. of 0: 59895\n",
      "DeleteMedicine: shape of y is (673, 102).\n",
      "in SplitBothXy_Df of load_data, len of val_idx 134\n",
      "train_X.shape:  (539, 46)\n",
      "train_y.shape:  (539, 102)\n",
      "\n",
      "debug no, of 0 and 1 in y after Split trina val\n",
      "no. of 1 in train_y: 7069\n",
      "no. of 0 in train_y: 47909\n",
      "no. of 1 in val_y: 1682\n",
      "no. of 0 in val_y: 11986\n"
     ]
    }
   ],
   "source": [
    "del_med_under_thres = 0     # 於所有medical cases中出現次數少於此數的藥->整col 刪除\n",
    "                            # if set to 250 -> will leave 10 medicine\n",
    "\n",
    "total_med = 102\n",
    "only_train_1st_n = None\n",
    "\n",
    "(X_np, X_val_np, train_y, val_y, \n",
    "  num_col_x, num_1_valy, num_0_valy) = load_data.load_data_for_1_med_with_debug(del_med_thres=del_med_under_thres, \n",
    "                                                                                n=only_train_1st_n, \n",
    "                                                                                file_name='simplified_data/data_v2_grouped_symptom.csv')\n",
    "\n",
    "assert(isinstance(X_np, np.ndarray))\n",
    "assert(isinstance(X_val_np, np.ndarray))\n",
    "assert(isinstance(train_y, pd.DataFrame))\n",
    "assert(isinstance(val_y, pd.DataFrame))\n",
    "assert(isinstance(num_col_x, int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type checking  you can run this if you suspect data type \n",
    "# else can skip this cell\n",
    "\n",
    "\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "# print(val_y)\n",
    "# checking\n",
    "# 1. Count occurrence of na, int, float, str in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # class_weights = compute_sample_weight(class_weight='balanced', y=train_y)\n",
    "# # class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # print(class_weight_dict)\n",
    "\n",
    "# train_y_np = np.array(train_y)\n",
    "\n",
    "# num_labels = train_y_np.shape[1]\n",
    "# class_weight_dic = {}\n",
    "\n",
    "# for i in range(num_labels):\n",
    "#     unique_values, counts = np.unique(train_y_np[:, i], return_counts=True)\n",
    "#     value_frequency_dict = dict(zip(unique_values, counts))\n",
    "#     total = value_frequency_dict[0] + value_frequency_dict[1]\n",
    "#     class_weight_dic[i] = {0: (value_frequency_dict[1] / total), 1: 5 * (value_frequency_dict[0] / total)} \n",
    "#     # f1: 0.35\n",
    "#     # class_weight_dic[i] = {0: (total / value_frequency_dict[0]), 1: 5 * (total / value_frequency_dict[1])}\n",
    "\n",
    "# print(class_weight_dic)\n",
    "# class_weight_list = np.array(class_weight_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense (units=16, input_shape=(num_col_x,), activation='relu'),\n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=2, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "train medicine-by-medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_df_dict={}   # dict of df\n",
    "#loss_acc_dict={}    # dict of loss and accuracy of each medicine\n",
    "accuracy_dict={}    # dict of accuracy of each medicine\n",
    "raw_prediction_train_dict={}   # dict of raw prediction of train set\n",
    "raw_prediction_val_dict={}   # dict of raw prediction of val set\n",
    "histories={}\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "#for i in range(2):\n",
    "\n",
    "    chosen_col = train_y.iloc[:,i].copy()\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(train_y))\n",
    "    print(f\"processing the {i+1} of {train_y.shape[1]} medicine: { chosen_col.name}\")\n",
    "\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "\n",
    "    y_val_chosen_col = val_y.iloc[:,i].copy()\n",
    "    assert(isinstance(y_val_chosen_col, pd.Series))\n",
    "    assert(len(y_val_chosen_col) == len(val_y))\n",
    "    #y_val_chosen_col_np = y_val_chosen_col.values.astype('float64')\n",
    "\n",
    "    \n",
    "    # Early stop\n",
    "    #early_stopping = EarlyStopping(monitor='loss', patience=100, restore_best_weights=True)\n",
    "\n",
    "    \n",
    "    # fit model for this medicine\n",
    "    history =  model.fit(\n",
    "        x=X_np,\n",
    "        y=chosen_y_np,\n",
    "        # class_weight=class_weight_dic[i],\n",
    "        epochs=100,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        #callbacks=[early_stopping]\n",
    "    )   # batch_size=32 if not specified\n",
    "    \n",
    "    # make prediction for this medicine\n",
    "    predictions_val_set = model.predict(X_val_np)\n",
    "    if(i==1):\n",
    "        print(\"type of predictions val: \" ,type(predictions_val_set))\n",
    "        print(\"predictions.shape:\", predictions_val_set.shape)\n",
    "        \n",
    "    raw_prediction_val_dict[chosen_col.name] = predictions_val_set  # save raw result np of val set to dict\n",
    "    \n",
    "\n",
    "    predictions_train_set = model.predict(X_np)   # predict against training set for diagonse overfit or underfit\n",
    "    raw_prediction_train_dict[chosen_col.name] = predictions_train_set  # save raw result np of train set to dict\n",
    "    histories[chosen_col.name] = history.history    # save history \n",
    "\n",
    "print(\"training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( type(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple check\n",
    "# total_zeros = np.sum(predictions_val_set == 0)\n",
    "# print(\"total no. of 0 in prediction of val\", total_zeros)\n",
    "# total_ones = np.sum(predictions_val_set == 1)\n",
    "# print(\"total no. of 1 in prediction of val\", total_ones)\n",
    "# total_zeros = np.sum(predictions_train_set == 0)\n",
    "# print(\"total no. of 0 in prediction of train\", total_zeros)\n",
    "# total_ones = np.sum(predictions_train_set == 1)\n",
    "# print(\"total no. of 1 in prediction of train\", total_ones)\n",
    "# print(np.count_nonzero(predictions_val_set))\n",
    "# print(np.count_nonzero(predictions_train_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal TP, FP, TN, FN for train set\n",
    "total_tp_train=0\n",
    "total_fp_train =0\n",
    "total_tn_train =0\n",
    "total_fn_train =0\n",
    "\n",
    "predicted_value_index = None\n",
    "ground_truth_index = None\n",
    "\n",
    "for key, arr in raw_prediction_train_dict.items():\n",
    "    df_tmp = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "    df_tmp[\"predicted value\"] = np.where(df_tmp[\"predicted as 0\"] > df_tmp[\"predicted as 1\"], 0, 1)\n",
    "    col_num = train_y.columns.get_loc(key)\n",
    "    df_tmp[\"ground truth\"] = train_y.iloc[:,col_num].copy().values\n",
    "    if predicted_value_index is None:\n",
    "        predicted_value_index = df_tmp.columns.get_loc('predicted value')\n",
    "    if ground_truth_index is None:\n",
    "        ground_truth_index = df_tmp.columns.get_loc('ground truth')\n",
    "\n",
    "    total_tp_train += ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    total_fp_train += ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    total_fn_train += ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    total_tn_train += ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 0)).sum()\n",
    "\n",
    "\n",
    "overall_f1_train = 2 * total_tp_train / (2 * total_tp_train + total_fp_train + total_fn_train) if (2 * total_tp_train + total_fp_train + total_fn_train) != 0 else 0\n",
    "\n",
    "train_set_acc = {}\n",
    "train_set_acc[\"TP\"]=total_tp_train\n",
    "train_set_acc[\"FP\"]=total_fp_train\n",
    "train_set_acc[\"FN\"]=total_fn_train\n",
    "train_set_acc[\"TN\"]=total_tn_train\n",
    "# no. of medical case in train set = # row in train_x * # col in train_y or val_y\n",
    "med_case_train = (X_np.shape[0] *  val_y.shape[1])\n",
    "train_set_acc[\"TP_percentage\"] = total_tp_train / med_case_train\n",
    "train_set_acc[\"FP_percentage\"] = total_fp_train / med_case_train\n",
    "train_set_acc[\"FN_percentage\"] = total_fn_train / med_case_train\n",
    "train_set_acc[\"TN_percentage\"] = total_tn_train / med_case_train\n",
    "train_set_acc[\"overall_f1\"] = overall_f1_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_acc[\"precision\"] = total_tp_train / (total_tp_train + total_fp_train) if (total_tp_train + total_fp_train) != 0 else 0\n",
    "train_set_acc[\"recall\"] = total_tp_train / (total_tp_train + total_fn_train) if (total_tp_train + total_fn_train) != 0 else 0\n",
    "\n",
    "# precision = 判斷為true之中有多少是對的 = TP / (TP + FP) \n",
    "# recall  =  實際為true之中有多少被找到  = TP / (TP + FN) \n",
    "\n",
    "try:\n",
    "    assert((total_tp_train + total_fp_train + total_fn_train + total_tn_train) == med_case_train)\n",
    "except:\n",
    "    print(\"assertion error for calculation check of train set\")\n",
    "    print(f\"tp+fp+tn+fn={total_tp_train + total_fp_train + total_fn_train + total_tn_train} , total med case={med_case_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# put prediction into df for val set\n",
    "\n",
    "for key, arr in raw_prediction_val_dict.items():\n",
    "    df_predictions = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "    df_predictions[\"predicted value\"] = np.where(df_predictions[\"predicted as 0\"] > df_predictions[\"predicted as 1\"], 0, 1)\n",
    "    col_num = val_y.columns.get_loc(key)\n",
    "    df_predictions[\"ground truth\"] = val_y.iloc[:,col_num].copy().values\n",
    "    df_predictions[\"is_correct\"] = df_predictions[\"predicted value\"] == df_predictions[\"ground truth\"]\n",
    "    accuracy = df_predictions[\"is_correct\"].mean()\n",
    "    accuracy_dict[key] = accuracy  \n",
    "    result_df_dict[key] = df_predictions\n",
    "    result_df_dict[key] = df_predictions\n",
    "    print(tabulate(df_predictions, headers='keys',tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_value_index = None\n",
    "ground_truth_index = None\n",
    "\n",
    "acc_each_med={}\n",
    "total_tp=0\n",
    "total_fp=0\n",
    "total_fn=0\n",
    "total_tn=0\n",
    "\n",
    "# df.columns.tolist()= pick list of column names\n",
    "\n",
    "for i in train_y.columns.tolist(): # build empty dict for holding TP, FP, FN, TN, accuracy, f1_score of each medicine\n",
    "    acc_each_med[i]={\"TP\": None, \"FP\": None, \"FN\": None, \"TN\": None, \"accuracy\": None, \"f1_score\": None}\n",
    "\n",
    "# calculate f1 score of for each medicine\n",
    "\n",
    "for key, df in result_df_dict.items():\n",
    "    # create 'TP/FP/TN/TN/FN' column\n",
    "\n",
    "    if predicted_value_index is None:\n",
    "        predicted_value_index = df.columns.get_loc('predicted value')\n",
    "    if ground_truth_index is None:\n",
    "        ground_truth_index = df.columns.get_loc('ground truth')\n",
    "\n",
    "\n",
    "    df['TP/FP/TN/TN/FN'] = df.apply(lambda row: 'TP' if ( row.iloc[predicted_value_index] and row.iloc[ground_truth_index] ) else \n",
    "                                 ('FP' if (row.iloc[predicted_value_index]  and (not row.iloc[ground_truth_index] )) else \n",
    "                                  ('FN' if (( not row.iloc[predicted_value_index] ) and row.iloc[ground_truth_index] ) else \n",
    "                                   ( 'TN' if ((not row.iloc[predicted_value_index]) and (not row.iloc[ground_truth_index]) ) else 'UN'))), axis=1)\n",
    "\n",
    "    # count occurrences of 'TP', 'FP', 'FN', and 'TN' of *this medicine*\n",
    "    counts = df['TP/FP/TN/TN/FN'].value_counts()\n",
    "    # get number of 'TP', 'FP', 'FN', and 'TN'\n",
    "    num_tp = counts.get('TP', 0)\n",
    "    num_fp = counts.get('FP', 0)\n",
    "    num_fn = counts.get('FN', 0)\n",
    "    num_tn = counts.get('TN', 0)\n",
    "\n",
    "    f1_score = 2 * num_tp / (2 * num_tp + num_fp + num_fn) if (2 * num_tp + num_fp + num_fn) > 0 else 0\n",
    "    acc_each_med[key][\"TP\"] = num_tp\n",
    "    acc_each_med[key][\"FP\"] = num_fp\n",
    "    acc_each_med[key][\"FN\"] = num_fn\n",
    "    acc_each_med[key][\"TN\"] = num_tn\n",
    "    acc_each_med[key][\"accuracy\"] = accuracy_dict[key].item()\n",
    "    acc_each_med[key][\"f1_score\"] = f1_score\n",
    "\n",
    "    total_tp += num_tp\n",
    "    total_fp += num_fp\n",
    "    total_fn += num_fn\n",
    "    total_tn += num_tn\n",
    "\n",
    "    ### debug messages\n",
    "    print(\"in for loop of result_df_dict\")\n",
    "    print(f\"processing : {key} ....\")\n",
    "    print(f\"TP of {key}: {num_tp}\", sep=\"\\t\")\n",
    "    print(f\"FP of {key}: {num_fp}\", sep=\"\\t\")\n",
    "    print(f\"FN of {key}: {num_fn}\", sep=\"\\t\")\n",
    "    print(f\"TN of {key}: {num_tn}\", sep=\"\\t\")\n",
    "    print(f\"f1_score of {key}: {f1_score}\")\n",
    "    print(f\"total tp now is: {total_tp}\"  , sep=\"\\t\")\n",
    "    print(f\"total fp now is: {total_fp}\"  , sep=\"\\t\")\n",
    "    print(f\"total fn now is: {total_fn}\"  , sep=\"\\t\")\n",
    "    print(f\"total tn now is: {total_tn}\"  , sep=\"\\t\")\n",
    "\n",
    "overall_f1 = 2 * total_tp / (2 * total_tp + total_fp + total_fn) if (2 * total_tp + total_fp + total_fn) > 0 else 0\n",
    "total_med_case =  len(val_y)* val_y.shape[1]\n",
    "print(\" \\n*****   end of for loop   *****\\n\")\n",
    "print(\"now doing calculation checking...\")\n",
    "mean_accuracy = statistics.mean(accuracy_dict.values())\n",
    "acc_each_med[\"overall\"]={}\n",
    "acc_each_med[\"overall\"][\"TP\"] = total_tp\n",
    "acc_each_med[\"overall\"][\"FP\"] = total_fp\n",
    "acc_each_med[\"overall\"][\"FN\"] = total_fn\n",
    "acc_each_med[\"overall\"][\"TN\"] = total_tn\n",
    "acc_each_med[\"overall\"][\"TP_percentage\"] ={total_tp/total_med_case}\n",
    "acc_each_med[\"overall\"][\"FP_percentage\"] ={total_fp/total_med_case}\n",
    "acc_each_med[\"overall\"][\"FN_percentage\"] ={total_fn/total_med_case}\n",
    "acc_each_med[\"overall\"][\"TN_percentage\"] ={total_tn/total_med_case}\n",
    "acc_each_med[\"overall\"][\"f1_score\"]=overall_f1\n",
    "acc_each_med[\"overall\"][\"mean_accuracy\"]=mean_accuracy\n",
    "acc_each_med[\"overall\"][\"precision\"] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) != 0 else 0\n",
    "acc_each_med[\"overall\"][\"recall\"] = total_tp / (total_tp + total_fn) if (total_tp + total_fn) != 0 else 0\n",
    "\n",
    "\n",
    "try:\n",
    "    assert((total_tp + total_fp + total_fn + total_tn) == total_med_case)\n",
    "    assert((total_tp + total_fn)== num_1_valy)  # TP+FN=all 1 in val_y\n",
    "    assert((total_fp + total_tn)== num_0_valy)  # TN+FP=all 0 in val_y\n",
    "    \n",
    "except:\n",
    "    print(\"wrong calculation!\")\n",
    "    print(f\"otal_tp + total_fp + total_fn + total_tn= {total_tp + total_fp + total_fn + total_tn}\", sep=\"\\t\")\n",
    "    print(f\"total_med_case= {total_med_case}\")\n",
    "    print(f\"total_tp + total_fn= {total_tp + total_fn}\", sep=\"\\t\")\n",
    "    print(f\"num_1_valy= {num_1_valy}\")\n",
    "    print(f\"total_fp + total_tn= {total_fp + total_tn}\", sep=\"\\t\")\n",
    "    print(f\"num_0_valy= {num_0_valy}\")\n",
    "else:\n",
    "    print(\"checking passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"total tp: {total_tp}, \\t percentage = {total_tp/total_med_case}\" )\n",
    "print(f\"total fp: {total_fp}, \\t percentage = {total_fp/total_med_case}\" )\n",
    "print(f\"total fn: {total_fn}, \\t percentage = {total_fn/total_med_case}\" )\n",
    "print(f\"total tn: {total_tn}, \\t percentage = {total_tn/total_med_case}\" )\n",
    "print(f\"overall f1 score: {overall_f1}\")\n",
    "print(\"mean accuracy of all medicine: \", mean_accuracy)\n",
    "print(\"precision: \",acc_each_med[\"overall\"][\"precision\"])\n",
    "print(\"recall: \", acc_each_med[\"overall\"][\"recall\"])\n",
    "print(acc_each_med)         # can't use json.dumps as there are np.int64\n",
    "# precision = 判斷為true之中有多少是對的 = TP / (TP + FP) \n",
    "# recall  =  實際為true之中有多少被找到  = TP / (TP + FN)\n",
    "print(\"-----------------------\\n-----  training set accuracy  -----\")\n",
    "print(train_set_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_suffix = \"sympton_only_each_med_csv\"   # type the dir for you to remember where u save the result\n",
    "for key, df in result_df_dict.items():\n",
    "    print(f\"DataFrame for {key}:\")\n",
    "    #myutil.print_df(df)\n",
    "    myutil.df_to_csv(df, save_path=(\"./result/\"+file_path_suffix), file_prefix=key)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df for all f1_score of each medicine\n",
    "all_f1_df = pd.DataFrame([(key, val['f1_score'], (val['TP']+val['FN'])) for key, val in acc_each_med.items()], columns=['medicine', 'f1_score', 'TP+FN'])\n",
    "myutil.df_to_csv(all_f1_df, save_path=(\"./result/\"+file_path_suffix), file_prefix='all_f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save f1_score and TP/FP/TN/TN/FN\n",
    "\n",
    "spec_str = \"train with x that only has sympton, model layer: 16-32-32-2, epoch=100, batch_size=32,  learning rate: 0.001, activation=relu, num_med:all, del_med_under_thres=0\"\n",
    "# need to type this spec str each time to record the result\n",
    "\n",
    "file_path=\"./result/symptom_only/\"\n",
    "myutil.dict_to_txt(acc_each_med, save_path=file_path, \n",
    "                   file_prefix=\"accuracy_each_med\",\n",
    "                   textbox=spec_str)\n",
    "\n",
    "\n",
    "myutil.dict_to_txt(train_set_acc, save_path=file_path, \n",
    "                   file_prefix=\"accuracy_train_set\",\n",
    "                   textbox=\"train set\"+spec_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
