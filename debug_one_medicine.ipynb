{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change to load data from load_data.py\n",
    "\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data 處理:\n",
    "\n",
    "data v2 刪掉所有文字input\n",
    "\n",
    "deleted 回診,西藥\n",
    "\n",
    "## train 方法:\n",
    "\n",
    ".....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from tabulate import tabulate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "# custom import \n",
    "import my_utilities as myutil\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "\n",
    "1. 決定是否要delete 少於某threshold的藥\n",
    "2. 用load_data裡的`load_data_for_n_med` load data\n",
    "3. 如只train 頭n 個藥-> set n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ReadData\n",
      "type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n",
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols). First 10 data of X:\n",
      "    肺癌  胰臟癌  肝癌  腺癌  攝護腺癌  骨癌  淋巴癌  胃癌  腦瘤  肝炎\n",
      "1    0    0   0   0     0   0    0   0   0   0\n",
      "2    0    0   0   0     0   0    0   0   0   0\n",
      "3    0    0   0   0     0   0    0   0   0   0\n",
      "4    0    0   0   0     0   0    0   0   0   0\n",
      "5    0    0   0   0     0   0    0   0   0   0\n",
      "6    0    0   0   0     0   0    0   0   0   0\n",
      "7    0    0   0   0     0   0    0   0   0   0\n",
      "8    0    0   0   0     0   0    0   0   0   0\n",
      "9    0    0   0   0     0   0    0   0   0   0\n",
      "10   0    0   0   0     0   0    0   0   0   0\n",
      "Shape of y = (796 rows, 3 cols). First 10 data of y:\n",
      "    麻黃  桂枝  荊芥\n",
      "1    0   1   0\n",
      "2    0   1   0\n",
      "3    0   0   0\n",
      "4    0   1   0\n",
      "5    0   1   0\n",
      "6    0   0   0\n",
      "7    0   1   0\n",
      "8    0   1   0\n",
      "9    0   1   0\n",
      "10   0   1   0\n",
      "=======================\n",
      "\n",
      "in load_data_and_debug of load_data.py, random_seed= 2\n",
      "after SplitXY, total number of 0, 1 in y:\n",
      "no. of 1: 489\n",
      "no. of 0: 1899\n",
      "DeleteMedicine: shape of y is (796, 3).\n",
      "in SplitBothXy_Df of load_data, len of val_idx 159\n",
      "train_X.shape:  (637, 111)\n",
      "train_y.shape:  (637, 3)\n",
      "\n",
      "debug no, of 0 and 1 in y after Split trina val\n",
      "no. of 1 in train_y: 388\n",
      "no. of 0 in train_y: 1523\n",
      "no. of 1 in val_y: 101\n",
      "no. of 0 in val_y: 376\n"
     ]
    }
   ],
   "source": [
    "del_med_under_thres = 0     # 於所有medical cases中出現次數少於此數的藥->整col 刪除\n",
    "                            # if set to 250 -> will leave 10 medicine\n",
    "\n",
    "only_train_1st_n = 3\n",
    "(X_np, X_val_np, train_y, val_y,  num_col_x) = load_data.load_data_and_debug(random_seed=2, n=only_train_1st_n)\n",
    "\n",
    "\n",
    "assert(isinstance(X_np, np.ndarray))\n",
    "assert(isinstance(X_val_np, np.ndarray))\n",
    "assert(isinstance(train_y, pd.DataFrame))\n",
    "assert(isinstance(val_y, pd.DataFrame))\n",
    "assert(isinstance(num_col_x, int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0 and 1 checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in y: 0\n",
      "Number of str values in y: 0\n",
      "Number of int values in y: 477\n",
      "Number of float values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# data type checking  you can run this if you suspect data type \n",
    "# else can skip this cell\n",
    "\n",
    "\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "# print(val_y)\n",
    "# checking\n",
    "# 1. Count occurrence of na, int, float, str in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class_weights = compute_sample_weight(class_weight='balanced', y=train_y)\n",
    "# class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                1792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,458\n",
      "Trainable params: 3,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense (units=16, input_shape=(num_col_x,), activation='relu'),\n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=2, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "train medicine-by-medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing medicine: 麻黃\n",
      "5/5 [==============================] - 0s 14ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 桂枝\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 荊芥\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "training done.\n"
     ]
    }
   ],
   "source": [
    "all_res=[]\n",
    "result_df_dict={}   # dict of df\n",
    "#loss_acc_dict={}    # dict of loss and accuracy of each medicine\n",
    "accuracy_dict={}    # dict of accuracy of each medicine\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "#for i in range(2):\n",
    "    res = {}\n",
    "    res_list =[]\n",
    "    chosen_col = train_y.iloc[:,i].copy()\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(train_y))\n",
    "    print(\"processing medicine:\", chosen_col.name)\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "\n",
    "    y_val_chosen_col = val_y.iloc[:,i].copy()\n",
    "    assert(isinstance(y_val_chosen_col, pd.Series))\n",
    "    assert(len(y_val_chosen_col) == len(val_y))\n",
    "    #y_val_chosen_col_np = y_val_chosen_col.values.astype('float64')\n",
    "\n",
    "    # fit model for this medicine\n",
    "    model.fit(x=X_np, y=chosen_y_np,\n",
    "          batch_size=50, epochs=10, shuffle=True, verbose=0)\n",
    "    \n",
    "    # make prediction for this medicine\n",
    "    predictions = model.predict(X_val_np)\n",
    "    print(type(predictions))\n",
    "    print(\"predictions.shape:\", predictions.shape)\n",
    "    \n",
    "    df_predictions = pd.DataFrame(predictions, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "    df_predictions[\"predicted value\"] = np.where(df_predictions[\"predicted as 0\"] > df_predictions[\"predicted as 1\"], 0, 1)\n",
    "    df_predictions[\"ground truth\"] = y_val_chosen_col.values\n",
    "     # create new column that checks if \"predicted value\" is equal to \"ground truth\"\n",
    "    df_predictions[\"is_correct\"] = df_predictions[\"predicted value\"] == df_predictions[\"ground truth\"]\n",
    "\n",
    "    accuracy = df_predictions[\"is_correct\"].mean()\n",
    "    accuracy_dict[chosen_col.name] = accuracy  \n",
    "    result_df_dict[chosen_col.name] = df_predictions\n",
    "    \n",
    "\n",
    "print(\"training done.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in for loop of result_df_dict\n",
      "processing : 麻黃 ....\n",
      "TP of 麻黃: 3\n",
      "FP of 麻黃: 7\n",
      "FN of 麻黃: 17\n",
      "TN of 麻黃: 132\n",
      "f1_score of 麻黃: 0.2\n",
      "total tp now is: 3\n",
      "total fp now is: 7\n",
      "total fn now is: 17\n",
      "total tn now is: 132\n",
      "in for loop of result_df_dict\n",
      "processing : 桂枝 ....\n",
      "TP of 桂枝: 30\n",
      "FP of 桂枝: 34\n",
      "FN of 桂枝: 45\n",
      "TN of 桂枝: 50\n",
      "f1_score of 桂枝: 0.4316546762589928\n",
      "total tp now is: 33\n",
      "total fp now is: 41\n",
      "total fn now is: 62\n",
      "total tn now is: 182\n",
      "in for loop of result_df_dict\n",
      "processing : 荊芥 ....\n",
      "TP of 荊芥: 0\n",
      "FP of 荊芥: 0\n",
      "FN of 荊芥: 6\n",
      "TN of 荊芥: 153\n",
      "f1_score of 荊芥: 0.0\n",
      "total tp now is: 33\n",
      "total fp now is: 41\n",
      "total fn now is: 68\n",
      "total tn now is: 335\n",
      " \n",
      "*****   end of for loop   *****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_name = \"result/train_1_med_result\"\n",
    "predicted_value_index = None\n",
    "ground_truth_index = None\n",
    "\n",
    "acc_each_med={}\n",
    "total_tp=0\n",
    "total_fp=0\n",
    "total_fn=0\n",
    "total_tn=0\n",
    "\n",
    "# df.columns.tolist()= pick list of column names\n",
    "\n",
    "for i in train_y.columns.tolist(): # build empty dict for holding TP, FP, FN, TN, accuracy, f1_score of each medicine\n",
    "    acc_each_med[i]={\"TP\": None, \"FP\": None, \"FN\": None, \"TN\": None, \"accuracy\": None, \"f1_score\": None}\n",
    "\n",
    "# calculate f1 score of for each medicine\n",
    "\n",
    "for key, df in result_df_dict.items():\n",
    "    # create 'TP/FP/TN/TN/FN' column\n",
    "\n",
    "    if predicted_value_index is None:\n",
    "        predicted_value_index = df.columns.get_loc('predicted value')\n",
    "    if ground_truth_index is None:\n",
    "        ground_truth_index = df.columns.get_loc('ground truth')\n",
    "\n",
    "\n",
    "    df['TP/FP/TN/TN/FN'] = df.apply(lambda row: 'TP' if ( row.iloc[predicted_value_index] and row.iloc[ground_truth_index] ) else \n",
    "                                 ('FP' if (row.iloc[predicted_value_index]  and (not row.iloc[ground_truth_index] )) else \n",
    "                                  ('FN' if (( not row.iloc[predicted_value_index] ) and row.iloc[ground_truth_index] ) else \n",
    "                                   ( 'TN' if ((not row.iloc[predicted_value_index]) and (not row.iloc[ground_truth_index]) ) else 'UN'))), axis=1)\n",
    "\n",
    "    # count occurrences of 'TP', 'FP', 'FN', and 'TN' of *this medicine*\n",
    "    counts = df['TP/FP/TN/TN/FN'].value_counts()\n",
    "    # get number of 'TP', 'FP', 'FN', and 'TN'\n",
    "    num_tp = counts.get('TP', 0)\n",
    "    num_fp = counts.get('FP', 0)\n",
    "    num_fn = counts.get('FN', 0)\n",
    "    num_tn = counts.get('TN', 0)\n",
    "\n",
    "    f1_score = 2 * num_tp / (2 * num_tp + num_fp + num_fn)\n",
    "    acc_each_med[key][\"TP\"] = num_tp\n",
    "    acc_each_med[key][\"FP\"] = num_fp\n",
    "    acc_each_med[key][\"FN\"] = num_fn\n",
    "    acc_each_med[key][\"TN\"] = num_tn\n",
    "    acc_each_med[key][\"accuracy\"] = accuracy_dict[key].item()\n",
    "    acc_each_med[key][\"f1_score\"] = f1_score\n",
    "\n",
    "    total_tp += num_tp\n",
    "    total_fp += num_fp\n",
    "    total_fn += num_fn\n",
    "    total_tn += num_tn\n",
    "\n",
    "    ### debug messages\n",
    "    print(\"in for loop of result_df_dict\")\n",
    "    print(f\"processing : {key} ....\")\n",
    "    print(f\"TP of {key}: {num_tp}\", sep=\"\\t\")\n",
    "    print(f\"FP of {key}: {num_fp}\", sep=\"\\t\")\n",
    "    print(f\"FN of {key}: {num_fn}\", sep=\"\\t\")\n",
    "    print(f\"TN of {key}: {num_tn}\", sep=\"\\t\")\n",
    "    print(f\"f1_score of {key}: {f1_score}\")\n",
    "    print(f\"total tp now is: {total_tp}\"  , sep=\"\\t\")\n",
    "    print(f\"total fp now is: {total_fp}\"  , sep=\"\\t\")\n",
    "    print(f\"total fn now is: {total_fn}\"  , sep=\"\\t\")\n",
    "    print(f\"total tn now is: {total_tn}\"  , sep=\"\\t\")\n",
    "\n",
    "overall_f1 = 2 * total_tp / (2 * total_tp + total_fp + total_fn) \n",
    "total_med_case =  len(val_y)* only_train_1st_n\n",
    "print(\" \\n*****   end of for loop   *****\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tp: 33, \t percentage = 0.06918238993710692\n",
      "total fp: 41, \t percentage = 0.0859538784067086\n",
      "total fn: 68, \t percentage = 0.14255765199161424\n",
      "total tn: 335, \t percentage = 0.7023060796645703\n",
      "overall f1 score: 0.37714285714285717\n",
      "mean accuracy of all medicine:  0.7714884696016772\n",
      "{'麻黃': {'TP': 3, 'FP': 7, 'FN': 17, 'TN': 132, 'accuracy': 0.8490566037735849, 'f1_score': 0.2}, '桂枝': {'TP': 30, 'FP': 34, 'FN': 45, 'TN': 50, 'accuracy': 0.5031446540880503, 'f1_score': 0.4316546762589928}, '荊芥': {'TP': 0, 'FP': 0, 'FN': 6, 'TN': 153, 'accuracy': 0.9622641509433962, 'f1_score': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy = statistics.mean(accuracy_dict.values())\n",
    "\n",
    "print(f\"total tp: {total_tp}, \\t percentage = {total_tp/total_med_case}\" )\n",
    "print(f\"total fp: {total_fp}, \\t percentage = {total_fp/total_med_case}\" )\n",
    "print(f\"total fn: {total_fn}, \\t percentage = {total_fn/total_med_case}\" )\n",
    "print(f\"total tn: {total_tn}, \\t percentage = {total_tn/total_med_case}\" )\n",
    "print(f\"overall f1 score: {overall_f1}\")\n",
    "print(\"mean accuracy of all medicine: \", mean_accuracy)\n",
    "print(acc_each_med)         # can't use json.dumps as there are np.int64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for key, df in result_df_dict.items():\n",
    "    print(f\"DataFrame for {key}:\")\n",
    "    myutil.print_df(df)\n",
    "    myutil.df_to_csv(df, save_path=\"./result/top10med_result\", file_prefix=key)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total medical case:  159\n",
      "f1 score: 0.0\n",
      "TP: 0, percentage = 0.0\n",
      "FP: 0, percentage = 0.0\n",
      "FN: 6, percentage = 0.03773584905660377\n",
      "TN: 153, percentage = 0.9622641509433962\n",
      "mean accuracy:  0.7714884696016772\n",
      "{'麻黃': 0.8490566037735849, '桂枝': 0.5031446540880503, '荊芥': 0.9622641509433962}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot dump dictToPrint, use normal print\n",
      "accuracy_summary saved to ./result/accuracy/accuracy_summary_9.txt\n",
      "{'spec': 'epoch=10, del_med_thres=0, debug', 'f1': 0.0, 'TP': 0, 'FP': 0, 'FN': 6, 'TN': 153}\n"
     ]
    }
   ],
   "source": [
    "# save f1_score and TP/FP/TN/TN/FN\n",
    "\n",
    "file_path=\"./result/accuracy\"\n",
    "accu_f1_dict = {}\n",
    "accu_f1_dict['spec']=\"epoch=10, del_med_thres=0, debug\"\n",
    "accu_f1_dict['f1']=f1_score\n",
    "accu_f1_dict['TP']=num_tp\n",
    "accu_f1_dict['FP']=num_fp\n",
    "accu_f1_dict['FN']=num_fn\n",
    "accu_f1_dict['TN']=num_tn\n",
    "myutil.dict_to_txt(accu_f1_dict, save_path=file_path, file_prefix='accuracy_summary')\n",
    "print(accu_f1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo111111\n"
     ]
    }
   ],
   "source": [
    "print(\"foo111111\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
