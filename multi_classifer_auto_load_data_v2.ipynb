{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change to load data from load_data.py\n",
    "\n",
    "use simplified data 2\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data 處理:\n",
    "\n",
    "data v2 刪掉所有文字input\n",
    "\n",
    "deleted 回診,西藥\n",
    "\n",
    "## train 方法:\n",
    "\n",
    "1個model train所有藥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tabulate import tabulate\n",
    "\n",
    "# custom import \n",
    "import my_utilities as myutil\n",
    "from utility_file import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 決定要train 多少個藥 \n",
    "2. 決定是否要delete 少於某threshold的藥\n",
    "3. 用load_data裡的`load_data_for_n_med` load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ReadData\n",
      "type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n",
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols). First 10 data of X:\n",
      "    肺癌  胰臟癌  肝癌  腺癌  攝護腺癌  骨癌  淋巴癌  胃癌  腦瘤  肝炎\n",
      "1    0    0   0   0     0   0    0   0   0   0\n",
      "2    0    0   0   0     0   0    0   0   0   0\n",
      "3    0    0   0   0     0   0    0   0   0   0\n",
      "4    0    0   0   0     0   0    0   0   0   0\n",
      "5    0    0   0   0     0   0    0   0   0   0\n",
      "6    0    0   0   0     0   0    0   0   0   0\n",
      "7    0    0   0   0     0   0    0   0   0   0\n",
      "8    0    0   0   0     0   0    0   0   0   0\n",
      "9    0    0   0   0     0   0    0   0   0   0\n",
      "10   0    0   0   0     0   0    0   0   0   0\n",
      "Shape of y = (796 rows, 3 cols). First 10 data of y:\n",
      "    麻黃  桂枝  荊芥\n",
      "1    0   1   0\n",
      "2    0   1   0\n",
      "3    0   0   0\n",
      "4    0   1   0\n",
      "5    0   1   0\n",
      "6    0   0   0\n",
      "7    0   1   0\n",
      "8    0   1   0\n",
      "9    0   1   0\n",
      "10   0   1   0\n",
      "DeleteMedicine: shape of y is (796, 3).\n",
      "type of val_idx <class 'list'>\n",
      "type of train_idx <class 'list'>\n",
      "len of val_idx 159\n",
      "train_X.shape:  (637, 111)\n",
      "train_y.shape:  (637, 3)\n",
      "X transformed to np array\n",
      "type of X_np: <class 'numpy.ndarray'>\n",
      "shape of X_np: (637, 111)\n",
      "shape of train y: (637, 3)\n",
      "type of X_val_np: <class 'numpy.ndarray'>\n",
      "shape of X_val_np: (159, 111)\n",
      "number of col in (train) x: 111\n",
      "number of medicine in y:  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_med_to_train = None  # train 從第一隻藥數起多少隻\n",
    "del_med_under_thres = 250     # 於所有medical cases中出現次數少於此數的藥->整col 刪除\n",
    "                            # if set to 250 -> will leave 10 medicine\n",
    "\n",
    "(X_np, train_X, X_val_np, val_X, y_np, train_y, y_val_np, val_y, num_col_x, num_medic) = load_data.load_data_for_n_med(num_med_to_train, del_med_thres=del_med_under_thres)\n",
    "\n",
    "# data type check\n",
    "assert(isinstance(X_np, np.ndarray))\n",
    "assert(isinstance(X_val_np, np.ndarray))\n",
    "assert(isinstance(y_np, np.ndarray))\n",
    "assert(isinstance(y_val_np, np.ndarray))\n",
    "assert(isinstance( train_X, pd.DataFrame))\n",
    "assert(isinstance( val_X, pd.DataFrame))\n",
    "assert(isinstance(train_y , pd.DataFrame))\n",
    "assert(isinstance( val_y, pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in y: 0\n",
      "Number of str values in y: 0\n",
      "Number of int values in y: 477\n",
      "Number of float values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# data type checking  you can run this if you suspect data type \n",
    "# else can skip this cell\n",
    "\n",
    "\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "# print(val_y)\n",
    "# checking\n",
    "# 1. Count occurrence of na, int, float, str in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                1792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,435\n",
      "Trainable params: 2,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build and compile model\n",
    "\n",
    "# 最後一個Dense為output layer, unit必須=藥的數量in y\n",
    "# unit = number of neuron\n",
    "# input_shape = input (X) 有多少個feature = # col of X\n",
    "# 可任意加上layer\n",
    "model = Sequential([\n",
    "    Dense (units=16, input_shape=(num_col_x,), activation='relu'),\n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=num_medic, activation='sigmoid')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 - 1s - loss: 0.4550 - accuracy: 0.3281 - 901ms/epoch - 14ms/step\n",
      "Epoch 2/10\n",
      "64/64 - 0s - loss: 0.3820 - accuracy: 0.3171 - 126ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "64/64 - 0s - loss: 0.3526 - accuracy: 0.3391 - 127ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "64/64 - 0s - loss: 0.3183 - accuracy: 0.4223 - 128ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "64/64 - 0s - loss: 0.2823 - accuracy: 0.4835 - 127ms/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "64/64 - 0s - loss: 0.2578 - accuracy: 0.4725 - 128ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "64/64 - 0s - loss: 0.2222 - accuracy: 0.5306 - 124ms/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "64/64 - 0s - loss: 0.1979 - accuracy: 0.5463 - 126ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "64/64 - 0s - loss: 0.1719 - accuracy: 0.5322 - 127ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "64/64 - 0s - loss: 0.1560 - accuracy: 0.5856 - 130ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b75720d4c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開始train model\n",
    "# epochs = 要train多少次\n",
    "model.fit(x=X_np, y=y_np,\n",
    "    batch_size=10, epochs=10, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions = model.predict(X_val_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.shape: (159, 3)\n"
     ]
    }
   ],
   "source": [
    "col_names =[]\n",
    "for i in  train_y.columns.tolist():\n",
    "    i +='(predicted)'\n",
    "    col_names.append(i) \n",
    "\n",
    "\n",
    "# df_predictions = probability of \"has\" that medicine (有該藥材)\n",
    "df_predictions = pd.DataFrame(predictions, columns=train_y.columns.tolist())\n",
    "print(\"predictions.shape:\", predictions.shape)\n",
    "\n",
    "yes_thres = 0.5         # threshold for summarized as \"has that medicine\"\n",
    "\n",
    "# all_class_binary_prediction_df = predicted as 是否有該藥材\n",
    "df_binary_prediction_all_class = df_predictions.apply(lambda x: [1 if i > yes_thres else 0 for i in x])\n",
    "\n",
    "# pred_and_truth = predicted as 是否有該藥材, and ground truth\n",
    "# pred_and_truth = pd.concat([df_binary_prediction_all_class, val_y.iloc[:, :num_medic]], axis=1)\n",
    "# ^ this problematic, ignore this for now\n",
    "\n",
    "## checking\n",
    "assert(df_binary_prediction_all_class.shape==val_y.shape)\n",
    "\n",
    "na_count = df_binary_prediction_all_class.isna().sum().sum()\n",
    "str_count = df_binary_prediction_all_class[df_binary_prediction_all_class.map(type) == str].count().sum()\n",
    "int_count = df_binary_prediction_all_class[df_binary_prediction_all_class.map(type) == int].count().sum()\n",
    "float_count = df_binary_prediction_all_class[df_binary_prediction_all_class.map(type) == float].count().sum()\n",
    "\n",
    "try:\n",
    "    assert((float_count==0) and (na_count==0) and (str_count==0) and int_count )\n",
    "except:\n",
    "    print(\"Warning: there are wrong data type in df_binary_prediction_all_class!!!\\n\")\n",
    "    print(f\"Number of NA values in df_binary_prediction_all_class: {na_count}\")\n",
    "    print(f\"Number of str values in df_binary_prediction_all_class: {str_count}\")\n",
    "    print(f\"Number of int values in df_binary_prediction_all_class: {int_count}\")\n",
    "    print(f\"Number of float values in df_binary_prediction_all_class: {float_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           麻黃        桂枝            荊芥\n",
      "0    0.134945  0.525377  5.223298e-06\n",
      "1    0.564172  0.962018  6.629257e-05\n",
      "2    0.000211  0.004546  1.316787e-07\n",
      "3    0.180478  0.238800  2.385344e-02\n",
      "4    0.000859  0.999885  9.508275e-08\n",
      "..        ...       ...           ...\n",
      "154  0.089143  0.024988  3.302015e-06\n",
      "155  0.009679  0.980020  1.056044e-05\n",
      "156  0.002719  0.975505  1.352384e-06\n",
      "157  0.180478  0.238800  2.385344e-02\n",
      "158  0.013920  0.994370  3.842598e-04\n",
      "\n",
      "[159 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     麻黃  桂枝  荊芥\n",
      "0     0   1   0\n",
      "1     1   1   0\n",
      "2     0   0   0\n",
      "3     0   0   0\n",
      "4     0   1   0\n",
      "..   ..  ..  ..\n",
      "154   0   0   0\n",
      "155   0   1   0\n",
      "156   0   1   0\n",
      "157   0   0   0\n",
      "158   0   1   0\n",
      "\n",
      "[159 rows x 3 columns]\n",
      "df_binary_prediction_all_class\n",
      "\n",
      "+-----+--------+--------+--------+\n",
      "|     |   麻黃 |   桂枝 |   荊芥 |\n",
      "|-----+--------+--------+--------|\n",
      "|   0 |      0 |      1 |      0 |\n",
      "|   1 |      1 |      1 |      0 |\n",
      "|   2 |      0 |      0 |      0 |\n",
      "|   3 |      0 |      0 |      0 |\n",
      "|   4 |      0 |      1 |      0 |\n",
      "|   5 |      0 |      1 |      0 |\n",
      "|   6 |      0 |      0 |      0 |\n",
      "|   7 |      0 |      0 |      0 |\n",
      "|   8 |      0 |      0 |      0 |\n",
      "|   9 |      0 |      0 |      0 |\n",
      "|  10 |      0 |      0 |      0 |\n",
      "|  11 |      0 |      1 |      0 |\n",
      "|  12 |      0 |      0 |      0 |\n",
      "|  13 |      1 |      1 |      0 |\n",
      "|  14 |      0 |      0 |      0 |\n",
      "|  15 |      0 |      0 |      0 |\n",
      "|  16 |      0 |      0 |      0 |\n",
      "|  17 |      0 |      0 |      0 |\n",
      "|  18 |      0 |      0 |      0 |\n",
      "|  19 |      0 |      0 |      0 |\n",
      "|  20 |      0 |      1 |      0 |\n",
      "|  21 |      0 |      1 |      0 |\n",
      "|  22 |      0 |      1 |      0 |\n",
      "|  23 |      0 |      1 |      0 |\n",
      "|  24 |      0 |      1 |      0 |\n",
      "|  25 |      0 |      0 |      0 |\n",
      "|  26 |      0 |      0 |      0 |\n",
      "|  27 |      0 |      1 |      0 |\n",
      "|  28 |      0 |      0 |      0 |\n",
      "|  29 |      0 |      1 |      0 |\n",
      "|  30 |      0 |      0 |      0 |\n",
      "|  31 |      0 |      1 |      0 |\n",
      "|  32 |      0 |      0 |      0 |\n",
      "|  33 |      0 |      0 |      0 |\n",
      "|  34 |      0 |      1 |      0 |\n",
      "|  35 |      1 |      1 |      0 |\n",
      "|  36 |      1 |      0 |      0 |\n",
      "|  37 |      0 |      1 |      0 |\n",
      "|  38 |      0 |      1 |      0 |\n",
      "|  39 |      0 |      0 |      0 |\n",
      "|  40 |      0 |      1 |      0 |\n",
      "|  41 |      0 |      0 |      0 |\n",
      "|  42 |      0 |      1 |      0 |\n",
      "|  43 |      0 |      0 |      0 |\n",
      "|  44 |      1 |      1 |      0 |\n",
      "|  45 |      1 |      1 |      0 |\n",
      "|  46 |      0 |      1 |      0 |\n",
      "|  47 |      0 |      0 |      0 |\n",
      "|  48 |      0 |      0 |      0 |\n",
      "|  49 |      0 |      1 |      0 |\n",
      "|  50 |      0 |      1 |      0 |\n",
      "|  51 |      0 |      0 |      0 |\n",
      "|  52 |      0 |      0 |      0 |\n",
      "|  53 |      0 |      0 |      0 |\n",
      "|  54 |      0 |      1 |      0 |\n",
      "|  55 |      0 |      1 |      0 |\n",
      "|  56 |      1 |      1 |      0 |\n",
      "|  57 |      0 |      1 |      0 |\n",
      "|  58 |      0 |      0 |      0 |\n",
      "|  59 |      0 |      0 |      0 |\n",
      "|  60 |      0 |      0 |      0 |\n",
      "|  61 |      0 |      1 |      0 |\n",
      "|  62 |      0 |      0 |      0 |\n",
      "|  63 |      0 |      0 |      0 |\n",
      "|  64 |      0 |      0 |      0 |\n",
      "|  65 |      0 |      1 |      0 |\n",
      "|  66 |      0 |      0 |      0 |\n",
      "|  67 |      0 |      0 |      0 |\n",
      "|  68 |      0 |      0 |      0 |\n",
      "|  69 |      1 |      0 |      0 |\n",
      "|  70 |      0 |      1 |      0 |\n",
      "|  71 |      0 |      1 |      0 |\n",
      "|  72 |      0 |      1 |      0 |\n",
      "|  73 |      0 |      0 |      0 |\n",
      "|  74 |      0 |      0 |      0 |\n",
      "|  75 |      0 |      0 |      0 |\n",
      "|  76 |      0 |      1 |      0 |\n",
      "|  77 |      0 |      1 |      0 |\n",
      "|  78 |      0 |      1 |      0 |\n",
      "|  79 |      0 |      0 |      0 |\n",
      "|  80 |      0 |      1 |      0 |\n",
      "|  81 |      0 |      0 |      0 |\n",
      "|  82 |      1 |      1 |      0 |\n",
      "|  83 |      1 |      1 |      0 |\n",
      "|  84 |      0 |      1 |      0 |\n",
      "|  85 |      0 |      0 |      0 |\n",
      "|  86 |      0 |      1 |      0 |\n",
      "|  87 |      0 |      0 |      0 |\n",
      "|  88 |      0 |      0 |      0 |\n",
      "|  89 |      0 |      0 |      0 |\n",
      "|  90 |      0 |      0 |      0 |\n",
      "|  91 |      0 |      0 |      0 |\n",
      "|  92 |      0 |      0 |      0 |\n",
      "|  93 |      0 |      1 |      0 |\n",
      "|  94 |      0 |      1 |      0 |\n",
      "|  95 |      0 |      0 |      0 |\n",
      "|  96 |      0 |      0 |      0 |\n",
      "|  97 |      0 |      0 |      0 |\n",
      "|  98 |      0 |      0 |      0 |\n",
      "|  99 |      0 |      0 |      0 |\n",
      "| 100 |      0 |      1 |      0 |\n",
      "| 101 |      0 |      1 |      0 |\n",
      "| 102 |      1 |      1 |      0 |\n",
      "| 103 |      0 |      1 |      0 |\n",
      "| 104 |      0 |      1 |      0 |\n",
      "| 105 |      0 |      1 |      0 |\n",
      "| 106 |      0 |      1 |      0 |\n",
      "| 107 |      0 |      1 |      0 |\n",
      "| 108 |      0 |      1 |      0 |\n",
      "| 109 |      1 |      1 |      0 |\n",
      "| 110 |      0 |      1 |      0 |\n",
      "| 111 |      0 |      1 |      0 |\n",
      "| 112 |      0 |      1 |      0 |\n",
      "| 113 |      0 |      1 |      0 |\n",
      "| 114 |      0 |      0 |      0 |\n",
      "| 115 |      0 |      0 |      0 |\n",
      "| 116 |      0 |      0 |      0 |\n",
      "| 117 |      0 |      0 |      0 |\n",
      "| 118 |      0 |      0 |      0 |\n",
      "| 119 |      0 |      1 |      0 |\n",
      "| 120 |      0 |      0 |      0 |\n",
      "| 121 |      0 |      0 |      0 |\n",
      "| 122 |      0 |      0 |      0 |\n",
      "| 123 |      0 |      1 |      0 |\n",
      "| 124 |      0 |      1 |      0 |\n",
      "| 125 |      0 |      1 |      0 |\n",
      "| 126 |      0 |      1 |      0 |\n",
      "| 127 |      0 |      0 |      0 |\n",
      "| 128 |      0 |      1 |      0 |\n",
      "| 129 |      1 |      0 |      0 |\n",
      "| 130 |      0 |      1 |      0 |\n",
      "| 131 |      0 |      0 |      0 |\n",
      "| 132 |      0 |      1 |      0 |\n",
      "| 133 |      0 |      0 |      0 |\n",
      "| 134 |      0 |      0 |      0 |\n",
      "| 135 |      0 |      0 |      0 |\n",
      "| 136 |      0 |      1 |      0 |\n",
      "| 137 |      0 |      1 |      0 |\n",
      "| 138 |      0 |      0 |      0 |\n",
      "| 139 |      0 |      1 |      0 |\n",
      "| 140 |      0 |      0 |      0 |\n",
      "| 141 |      0 |      0 |      0 |\n",
      "| 142 |      0 |      0 |      0 |\n",
      "| 143 |      0 |      1 |      0 |\n",
      "| 144 |      0 |      1 |      0 |\n",
      "| 145 |      0 |      0 |      0 |\n",
      "| 146 |      0 |      1 |      0 |\n",
      "| 147 |      1 |      1 |      0 |\n",
      "| 148 |      0 |      1 |      0 |\n",
      "| 149 |      0 |      1 |      0 |\n",
      "| 150 |      0 |      0 |      0 |\n",
      "| 151 |      0 |      1 |      0 |\n",
      "| 152 |      0 |      0 |      0 |\n",
      "| 153 |      0 |      0 |      0 |\n",
      "| 154 |      0 |      0 |      0 |\n",
      "| 155 |      0 |      1 |      0 |\n",
      "| 156 |      0 |      1 |      0 |\n",
      "| 157 |      0 |      0 |      0 |\n",
      "| 158 |      0 |      1 |      0 |\n",
      "+-----+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_binary_prediction_all_class)\n",
    "myutil.print_df(df_binary_prediction_all_class, \"df_binary_prediction_all_class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Result ***\n",
      "\n",
      "total medical case:  159\n",
      "f1 score: 0.33659491193737767\n",
      "TP: 86, percentage = 0.18029350104821804\n",
      "FP: 7, percentage = 0.014675052410901468\n",
      "FN: 332, percentage = 0.6960167714884696\n",
      "TN: 52, percentage = 0.1090146750524109\n"
     ]
    }
   ],
   "source": [
    "# calculate true positive, true negative, false positive, false negative cell-by-cell \n",
    "# against the binary prediction result and val_y\n",
    "\n",
    "\n",
    "df_is_correct = df_binary_prediction_all_class.copy()\n",
    "\n",
    "# df1[col].combine(df2[col], lambda_fn )\n",
    "# ^ this means combine `col` of df1 & df2 by using the lambda_fn\n",
    "# in lambda pred, true, \n",
    "#       pred is cell in df_binary_prediction_all_class[col]\n",
    "#       true is cell in df_is_correct[column]\n",
    "\n",
    "for column in df_is_correct.columns:\n",
    "    df_is_correct[column] = df_binary_prediction_all_class[column].combine(\n",
    "            val_y[column], \n",
    "            lambda pred, true: 'TP' if (pred and true) else \n",
    "                           ('FP' if (pred and (not true)) else \n",
    "                            ('FN' if ((not pred) and true) else \n",
    "                             ('TN' if((not pred)and (not true)) else 'UN')\n",
    "                             )))\n",
    "\n",
    "counts = pd.Series(df_is_correct.values.flatten()).value_counts()  #.flatten()-> convert to 1D np arr\n",
    "                                                                    # pd.Series(arr)-> convert to pd.Series\n",
    "                                                                    #value_counts() -> count all unique occurence\n",
    "\n",
    "# calculate TP, FP, FN, TN, f1 score\n",
    "num_tp = counts.get('TP', 0)\n",
    "num_fp = counts.get('FP', 0)\n",
    "num_fn = counts.get('FN', 0)\n",
    "num_tn = counts.get('TN', 0)\n",
    "num_un = counts.get('UN', 0)\n",
    "\n",
    "f1_score = 2 * num_tp / (2 * num_tp + num_fp + num_fn)\n",
    "try:\n",
    "    assert((num_tp+num_fp+num_fn+num_tn)== len(val_y)* num_medic)\n",
    "except:\n",
    "    print(\"wrong calculation!\")\n",
    "    print(f\"there are {num_un} UN case!\")\n",
    "total_med_case =  len(val_y)* num_medic\n",
    "print(\"*** Result ***\\n\")\n",
    "print(\"total medical case: \" , len(val_y))\n",
    "print(\"f1 score:\", f1_score)\n",
    "print(f\"TP: {num_tp}, percentage = {num_tp/total_med_case}\")\n",
    "print(f\"FP: {num_fp}, percentage = {num_fp/total_med_case}\")\n",
    "print(f\"FN: {num_fn}, percentage = {num_fn/total_med_case}\")\n",
    "print(f\"TN: {num_tn}, percentage = {num_tn/total_med_case}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- df is correct  -----\n",
      "\n",
      "+-----+--------+--------+--------+\n",
      "|     | 麻黃   | 桂枝   | 荊芥   |\n",
      "|-----+--------+--------+--------|\n",
      "|   0 | FN     | TP     | FN     |\n",
      "|   1 | TP     | TP     | FN     |\n",
      "|   2 | FN     | FN     | FN     |\n",
      "|   3 | FN     | FN     | FN     |\n",
      "|   4 | FN     | TP     | FN     |\n",
      "|   5 | FN     | TP     | FN     |\n",
      "|   6 | TN     | TN     | TN     |\n",
      "|   7 | FN     | FN     | FN     |\n",
      "|   8 | FN     | FN     | FN     |\n",
      "|   9 | TN     | FN     | TN     |\n",
      "|  10 | TN     | FN     | TN     |\n",
      "|  11 | TN     | TP     | TN     |\n",
      "|  12 | FN     | FN     | FN     |\n",
      "|  13 | TP     | TP     | FN     |\n",
      "|  14 | TN     | FN     | TN     |\n",
      "|  15 | FN     | FN     | FN     |\n",
      "|  16 | FN     | FN     | FN     |\n",
      "|  17 | FN     | FN     | FN     |\n",
      "|  18 | TN     | FN     | TN     |\n",
      "|  19 | FN     | FN     | FN     |\n",
      "|  20 | FN     | TP     | FN     |\n",
      "|  21 | FN     | TP     | FN     |\n",
      "|  22 | FN     | TP     | FN     |\n",
      "|  23 | FN     | TP     | FN     |\n",
      "|  24 | FN     | TP     | TN     |\n",
      "|  25 | TN     | TN     | TN     |\n",
      "|  26 | FN     | FN     | FN     |\n",
      "|  27 | FN     | TP     | FN     |\n",
      "|  28 | FN     | FN     | FN     |\n",
      "|  29 | TN     | FP     | TN     |\n",
      "|  30 | FN     | FN     | FN     |\n",
      "|  31 | FN     | TP     | FN     |\n",
      "|  32 | FN     | FN     | FN     |\n",
      "|  33 | FN     | FN     | FN     |\n",
      "|  34 | TN     | TP     | TN     |\n",
      "|  35 | TP     | TP     | FN     |\n",
      "|  36 | TP     | FN     | FN     |\n",
      "|  37 | TN     | FP     | TN     |\n",
      "|  38 | FN     | TP     | FN     |\n",
      "|  39 | FN     | FN     | FN     |\n",
      "|  40 | FN     | TP     | FN     |\n",
      "|  41 | FN     | FN     | FN     |\n",
      "|  42 | FN     | TP     | FN     |\n",
      "|  43 | TN     | TN     | TN     |\n",
      "|  44 | TP     | TP     | FN     |\n",
      "|  45 | TP     | TP     | FN     |\n",
      "|  46 | FN     | TP     | FN     |\n",
      "|  47 | FN     | FN     | FN     |\n",
      "|  48 | FN     | FN     | FN     |\n",
      "|  49 | FN     | TP     | FN     |\n",
      "|  50 | FN     | TP     | FN     |\n",
      "|  51 | FN     | FN     | FN     |\n",
      "|  52 | FN     | FN     | FN     |\n",
      "|  53 | FN     | FN     | FN     |\n",
      "|  54 | FN     | TP     | FN     |\n",
      "|  55 | FN     | TP     | FN     |\n",
      "|  56 | TP     | TP     | FN     |\n",
      "|  57 | FN     | TP     | FN     |\n",
      "|  58 | FN     | FN     | FN     |\n",
      "|  59 | TN     | FN     | TN     |\n",
      "|  60 | FN     | FN     | FN     |\n",
      "|  61 | FN     | TP     | FN     |\n",
      "|  62 | FN     | FN     | FN     |\n",
      "|  63 | FN     | FN     | FN     |\n",
      "|  64 | FN     | FN     | FN     |\n",
      "|  65 | TN     | TP     | TN     |\n",
      "|  66 | FN     | FN     | FN     |\n",
      "|  67 | FN     | FN     | FN     |\n",
      "|  68 | FN     | FN     | FN     |\n",
      "|  69 | TP     | FN     | FN     |\n",
      "|  70 | FN     | TP     | FN     |\n",
      "|  71 | FN     | TP     | FN     |\n",
      "|  72 | FN     | TP     | FN     |\n",
      "|  73 | FN     | FN     | FN     |\n",
      "|  74 | FN     | FN     | FN     |\n",
      "|  75 | FN     | FN     | TN     |\n",
      "|  76 | FN     | TP     | FN     |\n",
      "|  77 | FN     | TP     | FN     |\n",
      "|  78 | FN     | TP     | FN     |\n",
      "|  79 | TN     | TN     | TN     |\n",
      "|  80 | FN     | TP     | FN     |\n",
      "|  81 | FN     | FN     | FN     |\n",
      "|  82 | TP     | TP     | FN     |\n",
      "|  83 | TP     | TP     | FN     |\n",
      "|  84 | FN     | TP     | FN     |\n",
      "|  85 | FN     | FN     | FN     |\n",
      "|  86 | FN     | TP     | FN     |\n",
      "|  87 | FN     | FN     | FN     |\n",
      "|  88 | FN     | FN     | FN     |\n",
      "|  89 | FN     | FN     | TN     |\n",
      "|  90 | FN     | FN     | FN     |\n",
      "|  91 | FN     | FN     | FN     |\n",
      "|  92 | FN     | FN     | FN     |\n",
      "|  93 | TN     | FP     | TN     |\n",
      "|  94 | FN     | TP     | FN     |\n",
      "|  95 | FN     | FN     | FN     |\n",
      "|  96 | FN     | FN     | FN     |\n",
      "|  97 | FN     | FN     | FN     |\n",
      "|  98 | FN     | FN     | FN     |\n",
      "|  99 | FN     | FN     | FN     |\n",
      "| 100 | FN     | TP     | FN     |\n",
      "| 101 | FN     | TP     | FN     |\n",
      "| 102 | FP     | TP     | TN     |\n",
      "| 103 | FN     | TP     | FN     |\n",
      "| 104 | FN     | TP     | FN     |\n",
      "| 105 | FN     | TP     | FN     |\n",
      "| 106 | TN     | FP     | TN     |\n",
      "| 107 | FN     | TP     | FN     |\n",
      "| 108 | FN     | TP     | FN     |\n",
      "| 109 | TP     | TP     | FN     |\n",
      "| 110 | FN     | TP     | FN     |\n",
      "| 111 | TN     | TP     | TN     |\n",
      "| 112 | FN     | TP     | FN     |\n",
      "| 113 | FN     | TP     | FN     |\n",
      "| 114 | FN     | FN     | FN     |\n",
      "| 115 | FN     | FN     | FN     |\n",
      "| 116 | FN     | FN     | FN     |\n",
      "| 117 | FN     | FN     | FN     |\n",
      "| 118 | FN     | FN     | FN     |\n",
      "| 119 | FN     | TP     | FN     |\n",
      "| 120 | TN     | FN     | TN     |\n",
      "| 121 | FN     | FN     | FN     |\n",
      "| 122 | FN     | FN     | FN     |\n",
      "| 123 | FN     | TP     | FN     |\n",
      "| 124 | FN     | TP     | FN     |\n",
      "| 125 | FN     | TP     | FN     |\n",
      "| 126 | FN     | TP     | FN     |\n",
      "| 127 | FN     | FN     | FN     |\n",
      "| 128 | FN     | TP     | FN     |\n",
      "| 129 | TP     | FN     | FN     |\n",
      "| 130 | FN     | TP     | FN     |\n",
      "| 131 | FN     | FN     | FN     |\n",
      "| 132 | FN     | TP     | FN     |\n",
      "| 133 | FN     | FN     | FN     |\n",
      "| 134 | FN     | FN     | FN     |\n",
      "| 135 | FN     | FN     | FN     |\n",
      "| 136 | FN     | TP     | FN     |\n",
      "| 137 | FN     | TP     | FN     |\n",
      "| 138 | FN     | FN     | TN     |\n",
      "| 139 | FN     | TP     | FN     |\n",
      "| 140 | FN     | FN     | FN     |\n",
      "| 141 | FN     | FN     | FN     |\n",
      "| 142 | FN     | FN     | FN     |\n",
      "| 143 | FN     | TP     | FN     |\n",
      "| 144 | FN     | TP     | FN     |\n",
      "| 145 | FN     | FN     | FN     |\n",
      "| 146 | FN     | TP     | FN     |\n",
      "| 147 | TP     | TP     | FN     |\n",
      "| 148 | FN     | TP     | FN     |\n",
      "| 149 | TN     | FP     | TN     |\n",
      "| 150 | TN     | TN     | TN     |\n",
      "| 151 | TN     | FP     | TN     |\n",
      "| 152 | FN     | FN     | FN     |\n",
      "| 153 | FN     | FN     | FN     |\n",
      "| 154 | FN     | FN     | FN     |\n",
      "| 155 | FN     | TP     | FN     |\n",
      "| 156 | FN     | TP     | FN     |\n",
      "| 157 | FN     | FN     | FN     |\n",
      "| 158 | FN     | TP     | FN     |\n",
      "+-----+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "# print df_is_correct\n",
    "myutil.print_df(df_is_correct, \"--- df is correct  -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probi_each_med saved to ./multi_med_prediction/probi_each_med.csv\n",
      "is_correct saved to ./multi_med_prediction/is_correct.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# \n",
    "myutil.df_to_csv(df_predictions, save_path=\"./result/multi_med_prediction\", file_prefix=\"probi_each_med\")\n",
    "myutil.df_to_csv(df_is_correct, save_path=\"./result/multi_med_prediction\", file_prefix=\"is_correct\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
