{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model for TCM Project (pseudo code)\n",
    "\n",
    "## Steps of CNN\n",
    "\n",
    "2. 文本處理\n",
    "\n",
    "    文本向量化：將文本數據轉換為數值表示形式，比如將中文文字轉換成詞向量或字符向量。\n",
    "    填充序列：確保所有文本序列的長度相同，如果需要，進行填充或截斷操作。\n",
    "\n",
    "3. 建立CNN模型\n",
    "\n",
    "    卷積層與池化層：設計卷積層和池化層來提取特徵並減少輸入的維度。\n",
    "    Flatten層與全連接層：將卷積池化後的特徵展平成向量，然後添加全連接層進行分類。\n",
    "\n",
    "4. 模型訓練與評估\n",
    "\n",
    "    編譯模型：選擇損失函數、優化器和評估指標。\n",
    "    訓練模型：使用已處理的數據訓練CNN模型，通常通過反向傳播算法來更新權重。\n",
    "    評估模型：使用測試集來評估模型的性能，看準確度或其他指標。\n",
    "\n",
    "5. 模型調整和優化\n",
    "\n",
    "    超參數調整：調整卷積層的數量、大小，池化層的選擇等，以提高模型性能。\n",
    "    防止過擬合：使用正則化方法，如dropout層，以防止模型在訓練集上表現好但在測試集上表現差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taliah\\miniconda3\\envs\\ml2\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(os.path.dirname(sys.executable))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(FILENAME):\n",
    "    data = pd.read_csv(FILENAME)\n",
    "    # Debug\n",
    "    print(\"ReadData:\")\n",
    "    print(f'Shape of data = ({data.shape[0]} rows, {data.shape[1]} cols).')\n",
    "    return data\n",
    "\n",
    "# 2. Change the columns with texts into numeric values using LabelEncoder.\n",
    "def TextConvert(data):\n",
    "   # Body status: 1~3, Diagnosis: 4~7\n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_columns = list(range(1, 8))\n",
    "    for i in categorical_columns:\n",
    "        data.iloc[:, i] = label_encoder.fit_transform(data.iloc[:, i])\n",
    "    return data\n",
    "\n",
    "\n",
    "# 3. Split the data into the status/diagnoses/symptoms and the prescriptions.\n",
    "def SplitXY(data):\n",
    "    # Body status: 1~3, Diagnosis: 4~7, Symptom: 11~124\n",
    "    # Prescription: 125~226\n",
    "    split_X = list(range(1, 8)) + list(range(11, 126))\n",
    "    split_Y = list(range(125, 227))\n",
    "    X = data.iloc[1:, split_X]\n",
    "    y = data.iloc[1:, split_Y]\n",
    "    # Debug\n",
    "    print(\"SplitXY:\")\n",
    "    print(f'Shape of X = ({X.shape[0]} rows, {X.shape[1]} cols). First 10 data of X:')\n",
    "    print(X.iloc[:10, :10])\n",
    "    print(f'Shape of y = ({y.shape[0]} rows, {y.shape[1]} cols). First 10 data of y:')\n",
    "    print(y.iloc[:10, :10])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 4. Delete the CHMs that are not often used enough.\n",
    "def DeleteMedicine(y):\n",
    "    threshold = 10\n",
    "    for col in y.columns:\n",
    "        if y[col].sum() < threshold:\n",
    "            y = y.drop(col, axis=1)\n",
    "            \n",
    "\n",
    "# 5. Split the data into training data and validation data.\n",
    "def SplitTrainValid(X, y):\n",
    "    state = 114514\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=state, stratify=y)\n",
    "    # Debug\n",
    "    print(\"SplitTrainValid:\")\n",
    "    print(f'shape of X_train is ({X_train.shape[0]}, {X_train.shape[1]}).')\n",
    "    print(f'shape of X_test is ({X_test.shape[0]}, {X_test.shape[1]}).')\n",
    "    print(f'shape of y_train is ({y_train.shape[0]}, {y_train.shape[1]}).')\n",
    "    print(f'shape of y_test is ({y_test.shape[0]}, {y_test.shape[1]}).')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReadData:\n",
      "Shape of data = (797 rows, 227 cols).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2 functions\n",
    "FILENAME = './process_data.csv'\n",
    "data = ReadData(FILENAME)\n",
    "data = TextConvert(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建序列模型\n",
    "model = Sequential()\n",
    "\n",
    "# 添加卷積層和池化層\n",
    "model.add(Conv2D(\n",
    "    filters=32, \n",
    "    kernel_size=(3, 3), \n",
    "    activation='relu', \n",
    "    input_shape=(height, width, channels)))\n",
    "\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# 添加更多卷積層和池化層（根據需要）\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "#Drop掉一定比例的神經元來避免Overfit的狀況\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 將特徵展平成一維向量\n",
    "model.add(Flatten())\n",
    "\n",
    "# 添加全連接層\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # num_classes是你的分類數量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# parameter to tune:\n",
    "# optimizer\n",
    "# Optimizer Selection\n",
    "# loss\n",
    "# Loss Function: The 'loss' argument defines the loss function used to compute the error between predicted and actual values\n",
    "# metrics \n",
    "# specifies the evaluation metrics used to monitor the model's performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示模型摘要\n",
    "model.summary()\n",
    "train_history = model.fit(......)\n",
    "\n",
    "#plot graph\n",
    "show_train_history(.....)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
