{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change to load data from load_data.py\n",
    "\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data 處理:\n",
    "\n",
    "data v2 刪掉所有文字input\n",
    "\n",
    "deleted 回診,西藥\n",
    "\n",
    "## train 方法:\n",
    "\n",
    ".....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from tabulate import tabulate\n",
    "\n",
    "# custom import \n",
    "import my_utilities as myutil\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "1. 決定是否要delete 少於某threshold的藥\n",
    "2. 用load_data裡的`load_data_for_n_med` load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ReadData\n",
      "type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n",
      "first_medicine:  麻黃\n",
      "first_medicine_idx:  113\n",
      "should be 麻黃\n",
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols). First 10 data of X:\n",
      "    肺癌  胰臟癌  肝癌  腺癌  攝護腺癌  骨癌  淋巴癌  胃癌  腦瘤  肝炎\n",
      "1    0    0   0   0     0   0    0   0   0   0\n",
      "2    0    0   0   0     0   0    0   0   0   0\n",
      "3    0    0   0   0     0   0    0   0   0   0\n",
      "4    0    0   0   0     0   0    0   0   0   0\n",
      "5    0    0   0   0     0   0    0   0   0   0\n",
      "6    0    0   0   0     0   0    0   0   0   0\n",
      "7    0    0   0   0     0   0    0   0   0   0\n",
      "8    0    0   0   0     0   0    0   0   0   0\n",
      "9    0    0   0   0     0   0    0   0   0   0\n",
      "10   0    0   0   0     0   0    0   0   0   0\n",
      "Shape of y = (796 rows, 102 cols). First 10 data of y:\n",
      "    麻黃  桂枝  荊芥  防風  細辛  白芷  生薑  辛夷  葛根  升麻\n",
      "1    0   1   0   0   1   1   0   0   0   0\n",
      "2    0   1   0   0   0   0   1   0   0   0\n",
      "3    0   0   0   0   0   0   0   0   0   0\n",
      "4    0   1   0   0   0   0   1   1   0   0\n",
      "5    0   1   0   0   0   0   0   0   0   0\n",
      "6    0   0   0   0   0   0   0   0   0   0\n",
      "7    0   1   0   0   0   0   0   0   0   0\n",
      "8    0   1   0   0   0   0   0   0   0   0\n",
      "9    0   1   0   0   1   0   0   0   0   0\n",
      "10   0   1   0   0   1   0   0   0   0   0\n",
      "DeleteMedicine: shape of y is (796, 102).\n",
      "type of val_idx <class 'list'>\n",
      "type of train_idx <class 'list'>\n",
      "len of val_idx 159\n",
      "train_X.shape:  (637, 111)\n",
      "train_y.shape:  (637, 102)\n",
      "X transformed to np array\n",
      "type of X_np: <class 'numpy.ndarray'>\n",
      "shape of X_np: (637, 111)\n",
      "shape of train y: (637, 102)\n",
      "type of X_val_np: <class 'numpy.ndarray'>\n",
      "shape of X_val_np: (159, 111)\n",
      "number of col in (train) x: 111\n",
      "number of medicine in y:  102\n"
     ]
    }
   ],
   "source": [
    "del_med_under_thres = 0     # 於所有medical cases中出現次數少於此數的藥->整col 刪除\n",
    "                            # if set to 250 -> will leave 10 medicine\n",
    "(X_np, X_val_np, train_y, val_y,  num_col_x) = load_data.load_data_for_1_med()\n",
    "\n",
    "assert(isinstance(X_np, np.ndarray))\n",
    "assert(isinstance(X_val_np, np.ndarray))\n",
    "assert(isinstance(train_y, pd.DataFrame))\n",
    "assert(isinstance(val_y, pd.DataFrame))\n",
    "assert(isinstance(num_col_x, int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in y: 0\n",
      "Number of str values in y: 0\n",
      "Number of int values in y: 16218\n",
      "Number of float values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# data type checking  you can run this if you suspect data type \n",
    "# else can skip this cell\n",
    "\n",
    "\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "# print(val_y)\n",
    "# checking\n",
    "# 1. Count occurrence of na, int, float, str in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_weights = compute_sample_weight(class_weight='balanced', y=train_y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                1792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,458\n",
      "Trainable params: 3,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense (units=16, input_shape=(num_col_x,), activation='relu'),\n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=32, activation='relu'), \n",
    "    Dense (units=2, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "train medicine-by-medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing medicine: 麻黃\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 桂枝\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 荊芥\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 防風\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 細辛\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 白芷\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 生薑\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 辛夷\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 葛根\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 升麻\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 柴胡\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 蟬蛻\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 石膏\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 知母\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 梔子\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 天花粉\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 夏枯草\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 決明子\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 生地\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 牡丹皮\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 連翹\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 射干\n",
      "5/5 [==============================] - 0s 7ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 黃連\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "predictions.shape: (159, 2)\n",
      "processing medicine: 黃芩\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(y_val_chosen_col) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_y))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#y_val_chosen_col_np = y_val_chosen_col.values.astype('float64')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# fit model for this medicine\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchosen_y_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <---- add class weight\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# make prediction for this medicine\u001b[39;00m\n\u001b[0;32m     27\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_np)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_res=[]\n",
    "result_df_dict={}   # dict of df\n",
    "#loss_acc_dict={}    # dict of loss and accuracy of each medicine\n",
    "accuracy_dict={}    # dict of accuracy of each medicine\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "#for i in range(2):\n",
    "    res = {}\n",
    "    res_list =[]\n",
    "    chosen_col = train_y.iloc[:,i].copy()\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(train_y))\n",
    "    print(\"processing medicine:\", chosen_col.name)\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "\n",
    "    y_val_chosen_col = val_y.iloc[:,i].copy()\n",
    "    assert(isinstance(y_val_chosen_col, pd.Series))\n",
    "    assert(len(y_val_chosen_col) == len(val_y))\n",
    "    #y_val_chosen_col_np = y_val_chosen_col.values.astype('float64')\n",
    "\n",
    "    # fit model for this medicine\n",
    "    model.fit(x=X_np, y=chosen_y_np,\n",
    "        class_weight=class_weight_dict,  # <---- add class weight\n",
    "          batch_size=50, epochs=100, shuffle=True, verbose=0)\n",
    "    \n",
    "    # make prediction for this medicine\n",
    "    predictions = model.predict(X_val_np)\n",
    "    print(type(predictions))\n",
    "    print(\"predictions.shape:\", predictions.shape)\n",
    "    \n",
    "    df_predictions = pd.DataFrame(predictions, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "    df_predictions[\"predicted value\"] = np.where(df_predictions[\"predicted as 0\"] > df_predictions[\"predicted as 1\"], 0, 1)\n",
    "    df_predictions[\"ground truth\"] = y_val_chosen_col.values\n",
    "     # create new column that checks if \"predicted value\" is equal to \"ground truth\"\n",
    "    df_predictions[\"is_correct\"] = df_predictions[\"predicted value\"] == df_predictions[\"ground truth\"]\n",
    "\n",
    "    accuracy = df_predictions[\"is_correct\"].mean()\n",
    "    accuracy_dict[chosen_col.name] = accuracy  \n",
    "    result_df_dict[chosen_col.name] = df_predictions\n",
    "    \n",
    "\n",
    "print(\"training done.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_name = \"result/train_1_med_result\"\n",
    "predicted_value_index = None\n",
    "ground_truth_index = None\n",
    "\n",
    "# calculate f1 score of for each medicine\n",
    "\n",
    "for key, df in result_df_dict.items():\n",
    "    # create 'TP/FP/TN/TN/FN' column\n",
    "\n",
    "    if predicted_value_index is None:\n",
    "        predicted_value_index = df.columns.get_loc('predicted value')\n",
    "    if ground_truth_index is None:\n",
    "        ground_truth_index = df.columns.get_loc('ground truth')\n",
    "\n",
    "\n",
    "    df['TP/FP/TN/TN/FN'] = df.apply(lambda row: 'TP' if row[predicted_value_index] == 1 and row[ground_truth_index] == 1 else \n",
    "                                 ('FP' if row[predicted_value_index] == 1 and row[ground_truth_index] == 0 else \n",
    "                                  ('FN' if row[predicted_value_index] == 0 and row[ground_truth_index] == 1 else 'TN')), axis=1)\n",
    "\n",
    "# count occurrences of 'TP', 'FP', 'FN', and 'TN'\n",
    "counts = df['TP/FP/TN/TN/FN'].value_counts()\n",
    "# get number of 'TP', 'FP', 'FN', and 'TN'\n",
    "num_tp = counts.get('TP', 0)\n",
    "num_fp = counts.get('FP', 0)\n",
    "num_fn = counts.get('FN', 0)\n",
    "num_tn = counts.get('TN', 0)\n",
    "\n",
    "f1_score = 2 * num_tp / (2 * num_tp + num_fp + num_fn)\n",
    "    \n",
    "for key, df in result_df_dict.items():\n",
    "    print(f\"DataFrame for {key}:\")\n",
    "    myutil.print_df(df)\n",
    "    myutil.df_to_csv(df, save_path=\"./result/top10med_result\", file_prefix=key)\n",
    "   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
