{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change to load data from load_data.py\n",
    "\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data source: ./simplified_data/simplified_data2.csv\n",
    "\n",
    "data 處理:\n",
    "\n",
    "data v2 刪掉所有文字input\n",
    "\n",
    "deleted 回診,西藥\n",
    "\n",
    "## train 方法:\n",
    "\n",
    ".....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import statistics\n",
    "from tabulate import tabulate\n",
    "\n",
    "# custom import \n",
    "from utility_file import my_utilities as myutil\n",
    "from utility_file import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "1. 決定是否要delete 少於某threshold的藥\n",
    "2. 用load_data裡的`load_data_for_n_med` load data\n",
    "3. 如只train 頭n 個藥-> set `only_train_1st_n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ReadData\n",
      "type of data: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of data = (797 rows, 215 cols).\n",
      "SplitXY:\n",
      "Shape of X = (796 rows, 111 cols). First 10 data of X:\n",
      "    肺癌  胰臟癌  肝癌  腺癌  攝護腺癌  骨癌  淋巴癌  胃癌  腦瘤  肝炎\n",
      "1    0    0   0   0     0   0    0   0   0   0\n",
      "2    0    0   0   0     0   0    0   0   0   0\n",
      "3    0    0   0   0     0   0    0   0   0   0\n",
      "4    0    0   0   0     0   0    0   0   0   0\n",
      "5    0    0   0   0     0   0    0   0   0   0\n",
      "6    0    0   0   0     0   0    0   0   0   0\n",
      "7    0    0   0   0     0   0    0   0   0   0\n",
      "8    0    0   0   0     0   0    0   0   0   0\n",
      "9    0    0   0   0     0   0    0   0   0   0\n",
      "10   0    0   0   0     0   0    0   0   0   0\n",
      "Shape of y = (796 rows, 3 cols). First 10 data of y:\n",
      "    麻黃  桂枝  荊芥\n",
      "1    0   1   0\n",
      "2    0   1   0\n",
      "3    0   0   0\n",
      "4    0   1   0\n",
      "5    0   1   0\n",
      "6    0   0   0\n",
      "7    0   1   0\n",
      "8    0   1   0\n",
      "9    0   1   0\n",
      "10   0   1   0\n",
      "=======================\n",
      "\n",
      "in load_data_for_1_med_with_debug of load_data.py, random_seed= None\n",
      "after SplitXY, total number of 0, 1 in y:\n",
      "no. of 1: 489\n",
      "no. of 0: 1899\n",
      "DeleteMedicine: shape of y is (796, 3).\n",
      "in SplitBothXy_Df of load_data, len of val_idx 159\n",
      "train_X.shape:  (637, 111)\n",
      "train_y.shape:  (637, 3)\n",
      "\n",
      "debug no, of 0 and 1 in y after Split trina val\n",
      "no. of 1 in train_y: 385\n",
      "no. of 0 in train_y: 1526\n",
      "no. of 1 in val_y: 104\n",
      "no. of 0 in val_y: 373\n"
     ]
    }
   ],
   "source": [
    "del_med_under_thres = 0     # 於所有medical cases中出現次數少於此數的藥->整col 刪除\n",
    "                            # if set to 250 -> will leave 10 medicine\n",
    "\n",
    "total_med = 102\n",
    "only_train_1st_n = 3\n",
    "\n",
    "(X_np, X_val_np, train_y, val_y, \n",
    "  num_col_x, num_1_valy, num_0_valy) = load_data.load_data_for_1_med_with_debug(del_med_thres=del_med_under_thres, n=only_train_1st_n)\n",
    "\n",
    "assert(isinstance(X_np, np.ndarray))\n",
    "assert(isinstance(X_val_np, np.ndarray))\n",
    "assert(isinstance(train_y, pd.DataFrame))\n",
    "assert(isinstance(val_y, pd.DataFrame))\n",
    "assert(isinstance(num_col_x, int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in y: 0\n",
      "Number of str values in y: 0\n",
      "Number of int values in y: 477\n",
      "Number of float values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# data type checking  you can run this if you suspect data type \n",
    "# else can skip this cell\n",
    "\n",
    "\n",
    "# myutil.print_df(val_y, \"---- y ----\")\n",
    "# print(val_y)\n",
    "# checking\n",
    "# 1. Count occurrence of na, int, float, str in y\n",
    "na_count = val_y.isna().sum().sum()\n",
    "str_count = val_y[val_y.map(type) == str].count().sum()\n",
    "int_count = val_y[val_y.map(type) == int].count().sum()\n",
    "float_count = val_y[val_y.map(type) == float].count().sum()\n",
    "\n",
    "print(f\"Number of NA values in y: {na_count}\")\n",
    "print(f\"Number of str values in y: {str_count}\")\n",
    "print(f\"Number of int values in y: {int_count}\")\n",
    "print(f\"Number of float values in y: {float_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # class_weights = compute_sample_weight(class_weight='balanced', y=train_y)\n",
    "# # class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # print(class_weight_dict)\n",
    "\n",
    "# train_y_np = np.array(train_y)\n",
    "\n",
    "# num_labels = train_y_np.shape[1]\n",
    "# class_weight_dic = {}\n",
    "\n",
    "# for i in range(num_labels):\n",
    "#     unique_values, counts = np.unique(train_y_np[:, i], return_counts=True)\n",
    "#     value_frequency_dict = dict(zip(unique_values, counts))\n",
    "#     total = value_frequency_dict[0] + value_frequency_dict[1]\n",
    "#     class_weight_dic[i] = {0: (value_frequency_dict[1] / total), 1: 5 * (value_frequency_dict[0] / total)} \n",
    "#     # f1: 0.35\n",
    "#     # class_weight_dic[i] = {0: (total / value_frequency_dict[0]), 1: 5 * (total / value_frequency_dict[1])}\n",
    "\n",
    "# print(class_weight_dic)\n",
    "# class_weight_list = np.array(class_weight_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                7168      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,314\n",
      "Trainable params: 9,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense (units=64, input_shape=(num_col_x,), activation='sigmoid'),\n",
    "    # Dropout(0.1),\n",
    "    # Dense (units=128, activation='softmax'), \n",
    "    # Dropout(0.1),\n",
    "    # Dense (units=64, activation='softmax'), \n",
    "    # Dropout(0.1),\n",
    "    Dense (units=32, activation='softmax'), \n",
    "    Dense (units=2, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.005),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "train medicine-by-medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing the 1 of 3 medicine: 麻黃\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# fit model for this medicine\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchosen_y_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# class_weight=class_weight_dic[i],\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# batch_size=32 if not specified\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# make prediction for this medicine\u001b[39;00m\n\u001b[0;32m     39\u001b[0m predictions_val_set \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_np)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\taliah\\miniconda3\\envs\\ml2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_res=[]\n",
    "result_df_dict={}   # dict of df\n",
    "#loss_acc_dict={}    # dict of loss and accuracy of each medicine\n",
    "accuracy_dict={}    # dict of accuracy of each medicine\n",
    "raw_prediction_train_dict={}   # dict of raw prediction of train set\n",
    "raw_prediction_val_dict={}   # dict of raw prediction of val set\n",
    "histories={}\n",
    "\n",
    "for i in range(train_y.shape[1]):\n",
    "#for i in range(2):\n",
    "  \n",
    "    chosen_col = train_y.iloc[:,i].copy()\n",
    "    assert(isinstance(chosen_col, pd.Series))\n",
    "    assert(len(chosen_col) == len(train_y))\n",
    "    print(f\"processing the {i+1} of {train_y.shape[1]} medicine: { chosen_col.name}\")\n",
    "    chosen_y_np = chosen_col.values.astype('float64')\n",
    "\n",
    "    y_val_chosen_col = val_y.iloc[:,i].copy()\n",
    "    assert(isinstance(y_val_chosen_col, pd.Series))\n",
    "    assert(len(y_val_chosen_col) == len(val_y))\n",
    "    #y_val_chosen_col_np = y_val_chosen_col.values.astype('float64')\n",
    "\n",
    "    \n",
    "    # Early stop\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=100, restore_best_weights=True)\n",
    "    \n",
    "    # fit model for this medicine\n",
    "    history = model.fit(\n",
    "        x=X_np,\n",
    "        y=chosen_y_np,\n",
    "        # class_weight=class_weight_dic[i],\n",
    "        epochs=2000,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )   # batch_size=32 if not specified\n",
    "    \n",
    "    # make prediction for this medicine\n",
    "    predictions_val_set = model.predict(X_val_np)\n",
    "    if(i==1):\n",
    "        print(\"type of predictions val: \" ,type(predictions_val_set))\n",
    "        print(\"predictions.shape:\", predictions_val_set.shape)\n",
    "\n",
    "    raw_prediction_val_dict[chosen_col.name] = predictions_val_set  # save raw result np of val set to dict\n",
    "    \n",
    "\n",
    "    predictions_train_set = model.predict(X_np)   # predict against training set for diagonse overfit or underfit\n",
    "    raw_prediction_train_dict[chosen_col.name] = predictions_train_set  # save raw result np of train set to dict\n",
    "    histories[chosen_col.name] = history.history    # save history \n",
    "\n",
    "print(\"training done.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal TP, FP, TN, FN for train set\n",
    "total_tp_train=0\n",
    "total_fp_train =0\n",
    "total_tn_train =0\n",
    "total_fn_train =0\n",
    "\n",
    "predicted_value_index = None\n",
    "ground_truth_index = None\n",
    "\n",
    "for key, arr in raw_prediction_train_dict.items():\n",
    "    df_tmp = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "    df_tmp[\"predicted value\"] = np.where(df_tmp[\"predicted as 0\"] > df_tmp[\"predicted as 1\"], 0, 1)\n",
    "    col_num = train_y.columns.get_loc(key)\n",
    "    df_tmp[\"ground truth\"] = train_y.iloc[:,col_num].copy().values\n",
    "    if predicted_value_index is None:\n",
    "        predicted_value_index = df_tmp.columns.get_loc('predicted value')\n",
    "    if ground_truth_index is None:\n",
    "        ground_truth_index = df_tmp.columns.get_loc('ground truth')\n",
    "\n",
    "    total_tp_train += ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    total_fp_train += ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 1)).sum()\n",
    "    total_fn_train += ((df_tmp['ground truth'] == 1) & (df_tmp['predicted value'] == 0)).sum()\n",
    "    total_tn_train += ((df_tmp['ground truth'] == 0) & (df_tmp['predicted value'] == 0)).sum()\n",
    "\n",
    "\n",
    "overall_f1_train = 2 * total_tp_train / (2 * total_tp_train + total_fp_train + total_fn_train) if (2 * total_tp_train + total_fp_train + total_fn_train) != 0 else 0\n",
    "\n",
    "train_set_acc = {}\n",
    "train_set_acc[\"TP\"]=total_tp_train\n",
    "train_set_acc[\"FP\"]=total_fp_train\n",
    "train_set_acc[\"FN\"]=total_fn_train\n",
    "train_set_acc[\"TN\"]=total_tn_train\n",
    "# no. of medical case in train set = # row in train_x * # col in train_y or val_y\n",
    "med_case_train = (X_np.shape[0] *  val_y.shape[1])\n",
    "train_set_acc[\"TP_percentage\"] = total_tp_train / med_case_train\n",
    "train_set_acc[\"FP_percentage\"] = total_fp_train / med_case_train\n",
    "train_set_acc[\"FN_percentage\"] = total_fn_train / med_case_train\n",
    "train_set_acc[\"TN_percentage\"] = total_tn_train / med_case_train\n",
    "train_set_acc[\"overall_f1\"] = overall_f1_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set_acc[\"precision\"] = total_tp_train / (total_tp_train + total_fp_train) if (total_tp_train + total_fp_train) != 0 else 0\n",
    "train_set_acc[\"recall\"] = total_tp_train / (total_tp_train + total_fn_train) if (total_tp_train + total_fn_train) != 0 else 0\n",
    "\n",
    "# precision = 判斷為true之中有多少是對的 = TP / (TP + FP) \n",
    "# recall  =  實際為true之中有多少被找到  = TP / (TP + FN) \n",
    "\n",
    "try:\n",
    "    assert((total_tp_train + total_fp_train + total_fn_train + total_tn_train) == med_case_train)\n",
    "except:\n",
    "    print(\"assertion error for calculation check of train set\")\n",
    "    print(f\"tp+fp+tn+fn={total_tp_train + total_fp_train + total_fn_train + total_tn_train} , total med case={med_case_train}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# put prediction into df for val set\n",
    "\n",
    "for key, arr in raw_prediction_val_dict.items():\n",
    "    df_predictions = pd.DataFrame(arr, columns=[\"predicted as 0\", \"predicted as 1\"])\n",
    "    df_predictions[\"predicted value\"] = np.where(df_predictions[\"predicted as 0\"] > df_predictions[\"predicted as 1\"], 0, 1)\n",
    "    col_num = val_y.columns.get_loc(key)\n",
    "    df_predictions[\"ground truth\"] = val_y.iloc[:,col_num].copy().values\n",
    "    df_predictions[\"is_correct\"] = df_predictions[\"predicted value\"] == df_predictions[\"ground truth\"]\n",
    "    accuracy = df_predictions[\"is_correct\"].mean()\n",
    "    accuracy_dict[key] = accuracy  \n",
    "    result_df_dict[key] = df_predictions\n",
    "    result_df_dict[key] = df_predictions\n",
    "    print(tabulate(df_predictions, headers='keys',tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_value_index = None\n",
    "ground_truth_index = None\n",
    "\n",
    "acc_each_med={}\n",
    "total_tp=0\n",
    "total_fp=0\n",
    "total_fn=0\n",
    "total_tn=0\n",
    "\n",
    "# df.columns.tolist()= pick list of column names\n",
    "\n",
    "for i in train_y.columns.tolist(): # build empty dict for holding TP, FP, FN, TN, accuracy, f1_score of each medicine\n",
    "    acc_each_med[i]={\"TP\": None, \"FP\": None, \"FN\": None, \"TN\": None, \"accuracy\": None, \"f1_score\": None}\n",
    "\n",
    "# calculate f1 score of for each medicine\n",
    "\n",
    "for key, df in result_df_dict.items():\n",
    "    # create 'TP/FP/TN/TN/FN' column\n",
    "\n",
    "    if predicted_value_index is None:\n",
    "        predicted_value_index = df.columns.get_loc('predicted value')\n",
    "    if ground_truth_index is None:\n",
    "        ground_truth_index = df.columns.get_loc('ground truth')\n",
    "\n",
    "\n",
    "    df['TP/FP/TN/TN/FN'] = df.apply(lambda row: 'TP' if ( row.iloc[predicted_value_index] and row.iloc[ground_truth_index] ) else \n",
    "                                 ('FP' if (row.iloc[predicted_value_index]  and (not row.iloc[ground_truth_index] )) else \n",
    "                                  ('FN' if (( not row.iloc[predicted_value_index] ) and row.iloc[ground_truth_index] ) else \n",
    "                                   ( 'TN' if ((not row.iloc[predicted_value_index]) and (not row.iloc[ground_truth_index]) ) else 'UN'))), axis=1)\n",
    "\n",
    "    # count occurrences of 'TP', 'FP', 'FN', and 'TN' of *this medicine*\n",
    "    counts = df['TP/FP/TN/TN/FN'].value_counts()\n",
    "    # get number of 'TP', 'FP', 'FN', and 'TN'\n",
    "    num_tp = counts.get('TP', 0)\n",
    "    num_fp = counts.get('FP', 0)\n",
    "    num_fn = counts.get('FN', 0)\n",
    "    num_tn = counts.get('TN', 0)\n",
    "\n",
    "    f1_score = 2 * num_tp / (2 * num_tp + num_fp + num_fn) if (2 * num_tp + num_fp + num_fn) > 0 else 0\n",
    "    acc_each_med[key][\"TP\"] = num_tp\n",
    "    acc_each_med[key][\"FP\"] = num_fp\n",
    "    acc_each_med[key][\"FN\"] = num_fn\n",
    "    acc_each_med[key][\"TN\"] = num_tn\n",
    "    acc_each_med[key][\"accuracy\"] = accuracy_dict[key].item()\n",
    "    acc_each_med[key][\"f1_score\"] = f1_score\n",
    "\n",
    "    total_tp += num_tp\n",
    "    total_fp += num_fp\n",
    "    total_fn += num_fn\n",
    "    total_tn += num_tn\n",
    "\n",
    "    ### debug messages\n",
    "    print(\"in for loop of result_df_dict\")\n",
    "    print(f\"processing : {key} ....\")\n",
    "    print(f\"TP of {key}: {num_tp}\", sep=\"\\t\")\n",
    "    print(f\"FP of {key}: {num_fp}\", sep=\"\\t\")\n",
    "    print(f\"FN of {key}: {num_fn}\", sep=\"\\t\")\n",
    "    print(f\"TN of {key}: {num_tn}\", sep=\"\\t\")\n",
    "    print(f\"f1_score of {key}: {f1_score}\")\n",
    "    print(f\"total tp now is: {total_tp}\"  , sep=\"\\t\")\n",
    "    print(f\"total fp now is: {total_fp}\"  , sep=\"\\t\")\n",
    "    print(f\"total fn now is: {total_fn}\"  , sep=\"\\t\")\n",
    "    print(f\"total tn now is: {total_tn}\"  , sep=\"\\t\")\n",
    "\n",
    "overall_f1 = 2 * total_tp / (2 * total_tp + total_fp + total_fn) if (2 * total_tp + total_fp + total_fn) > 0 else 0\n",
    "total_med_case =  len(val_y)* val_y.shape[1]\n",
    "print(\" \\n*****   end of for loop   *****\\n\")\n",
    "print(\"now doing calculation checking...\")\n",
    "mean_accuracy = statistics.mean(accuracy_dict.values())\n",
    "acc_each_med[\"overall\"]={}\n",
    "acc_each_med[\"overall\"][\"TP\"] = total_tp\n",
    "acc_each_med[\"overall\"][\"FP\"] = total_fp\n",
    "acc_each_med[\"overall\"][\"FN\"] = total_fn\n",
    "acc_each_med[\"overall\"][\"TN\"] = total_tn\n",
    "acc_each_med[\"overall\"][\"TP_percentage\"] ={total_tp/total_med_case}\n",
    "acc_each_med[\"overall\"][\"FP_percentage\"] ={total_fp/total_med_case}\n",
    "acc_each_med[\"overall\"][\"FN_percentage\"] ={total_fn/total_med_case}\n",
    "acc_each_med[\"overall\"][\"TN_percentage\"] ={total_tn/total_med_case}\n",
    "acc_each_med[\"overall\"][\"f1_score\"]=overall_f1\n",
    "acc_each_med[\"overall\"][\"mean_accuracy\"]=mean_accuracy\n",
    "acc_each_med[\"overall\"][\"precision\"] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) != 0 else 0\n",
    "acc_each_med[\"overall\"][\"recall\"] = total_tp / (total_tp + total_fn) if (total_tp + total_fn) != 0 else 0\n",
    "\n",
    "\n",
    "try:\n",
    "    assert((total_tp + total_fp + total_fn + total_tn) == total_med_case)\n",
    "    assert((total_tp + total_fn)== num_1_valy)  # TP+FN=all 1 in val_y\n",
    "    assert((total_fp + total_tn)== num_0_valy)  # TN+FP=all 0 in val_y\n",
    "    \n",
    "except:\n",
    "    print(\"wrong calculation!\")\n",
    "    print(f\"otal_tp + total_fp + total_fn + total_tn= {total_tp + total_fp + total_fn + total_tn}\", sep=\"\\t\")\n",
    "    print(f\"total_med_case= {total_med_case}\")\n",
    "    print(f\"total_tp + total_fn= {total_tp + total_fn}\", sep=\"\\t\")\n",
    "    print(f\"num_1_valy= {num_1_valy}\")\n",
    "    print(f\"total_fp + total_tn= {total_fp + total_tn}\", sep=\"\\t\")\n",
    "    print(f\"num_0_valy= {num_0_valy}\")\n",
    "else:\n",
    "    print(\"checking passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"total tp: {total_tp}, \\t percentage = {total_tp/total_med_case}\" )\n",
    "print(f\"total fp: {total_fp}, \\t percentage = {total_fp/total_med_case}\" )\n",
    "print(f\"total fn: {total_fn}, \\t percentage = {total_fn/total_med_case}\" )\n",
    "print(f\"total tn: {total_tn}, \\t percentage = {total_tn/total_med_case}\" )\n",
    "print(f\"overall f1 score: {overall_f1}\")\n",
    "print(\"mean accuracy of all medicine: \", mean_accuracy)\n",
    "print(\"precision: \",acc_each_med[\"overall\"][\"precision\"])\n",
    "print(\"recall: \", acc_each_med[\"overall\"][\"recall\"])\n",
    "print(acc_each_med)         # can't use json.dumps as there are np.int64\n",
    "# precision = 判斷為true之中有多少是對的 = TP / (TP + FP) \n",
    "# recall  =  實際為true之中有多少被找到  = TP / (TP + FN)\n",
    "print(\"-----------------------\\n-----  training set accuracy  -----\")\n",
    "print(train_set_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_suffix = \"each_med_csv\"   # type the dir for you to remember where u save the result\n",
    "for key, df in result_df_dict.items():\n",
    "    print(f\"DataFrame for {key}:\")\n",
    "    #myutil.print_df(df)\n",
    "    myutil.df_to_csv(df, save_path=(\"./result/\"+file_path_suffix), file_prefix=key)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df for all f1_score of each medicine\n",
    "all_f1_df = pd.DataFrame([(key, val['f1_score'], (val['TP']+val['FN'])) for key, val in acc_each_med.items()], columns=['medicine', 'f1_score', 'TP+FN'])\n",
    "myutil.df_to_csv(all_f1_df, save_path=(\"./result/\"+file_path_suffix), file_prefix='all_f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save f1_score and TP/FP/TN/TN/FN\n",
    "\n",
    "spec_str = \"model layer:  32-64-128-64-32 units, activation: relu, optimizer: Adam, learning rate: 0.001, epochs: 1000, batch_size: 32, num_med: all. del_med_under_thres: 0\"\n",
    "# need to type this spec str each time to record the result\n",
    "\n",
    "file_path=\"./result/1_med_accuracy\"\n",
    "myutil.dict_to_txt(acc_each_med, save_path=file_path, \n",
    "                   file_prefix=\"accuracy_each_med\",\n",
    "                   textbox=spec_str)\n",
    "\n",
    "\n",
    "myutil.dict_to_txt(train_set_acc, save_path=file_path, \n",
    "                   file_prefix=\"accuracy_train_set\",\n",
    "                   textbox=\"train set\"+spec_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
